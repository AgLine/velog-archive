# EKS HPA + Karpenter autoscailing

ê²Œì‹œì¼: 2025-08-26T06:24:11.336Z
ì‹œë¦¬ì¦ˆ: AWS

---

í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìš´ì˜í•˜ë‹¤ ë³´ë©´, **'ë¦¬ì†ŒìŠ¤ ê´€ë¦¬'**
ë¼ëŠ” í’€ë¦¬ì§€ ì•ŠëŠ” ìˆ™ì œì™€ ë§ˆì£¼í•˜ê²Œ ë©ë‹ˆë‹¤. 
ê°‘ì‘ìŠ¤ëŸ¬ìš´ íŠ¸ë˜í”½ í­ì¦ìœ¼ë¡œ ì„œë¹„ìŠ¤ê°€ ë§ˆë¹„ë˜ê±°ë‚˜, ë°˜ëŒ€ë¡œ í•œì‚°í•œ ì‹œê°„ì—ë„ ë¹„ì‹¼ ìì›ì„ ë‚­ë¹„í•˜ê³  ìˆì§€ëŠ” ì•Šìœ¼ì‹ ê°€ìš”? 
ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ì— ì‹¤íŒ¨í•˜ì—¬ ìƒˆë²½ì— ê¸´ê¸‰ ëŒ€ì‘ì„ í•˜ê±°ë‚˜, ë§¤ë‹¬ ë‚ ì•„ì˜¤ëŠ” ì²­êµ¬ì„œì— í•œìˆ¨ ì‰¬ëŠ” ê²½í—˜ì€ ì´ì œ ê·¸ë§Œí•  ë•Œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ê³ ë¯¼ì„ í•´ê²°í•  ì—´ì‡ ëŠ” ë°”ë¡œ **'ì˜¤í† ìŠ¤ì¼€ì¼ë§'** ì— ìˆìŠµë‹ˆë‹¤. 
ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ **HPA(Horizontal Pod Autoscaler)** ì™€ **Karpenter** ë¥¼ ì¡°í•©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë¶€í•˜ì— ë”°ë¼ íŒŒë“œ(Pod)ì™€ ë…¸ë“œ(Node)ê°€ ì•Œì•„ì„œ ëŠ˜ì–´ë‚˜ê³  ì¤„ì–´ë“œëŠ” **'ì™„ì „ ìë™í™”ëœ ì˜¤í† ìŠ¤ì¼€ì¼ë§'** ì‹œìŠ¤í…œì„ Terraformìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ê³µìœ í•©ë‹ˆë‹¤.

- **HPA (Horizontal Pod Autoscaler)** : ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ CPU, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê°ì§€í•˜ì—¬ íŒŒë“œ(Pod)ì˜ ê°œìˆ˜ë¥¼ ì‹ ì†í•˜ê²Œ ì¡°ì ˆí•©ë‹ˆë‹¤.
- **Karpenter** : HPAë¡œ ì¸í•´ ëŠ˜ì–´ë‚œ íŒŒë“œë¥¼ ë°°ì¹˜í•  ê³µê°„ì´ ë¶€ì¡±í•  ë•Œ, í•„ìš”í•œ ì‚¬ì–‘ì˜ ë…¸ë“œ(EC2 ì¸ìŠ¤í„´ìŠ¤)ë¥¼ ì¦‰ì‹œ ìƒì„±í•˜ê³ , ë°˜ëŒ€ë¡œ ë¶ˆí•„ìš”í•´ì§„ ë…¸ë“œëŠ” ì œê±°í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.


# ì „ì²´ì•„í‚¤í…ì³
1. Terraformì„ ì‚¬ìš©í•˜ì—¬ AWSì˜ ê¸°ë³¸ ì¸í”„ë¼(VPC, Subnet, NAT Gateway, Bastion Host, RDS)ì™€ EKS í´ëŸ¬ìŠ¤í„°ë¥¼ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.

2. Bastion Hostë¥¼ í†µí•´ EKS í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•˜ì—¬ AWS Load Balancer Controller, Metrics Server, Karpenterë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

3. ì• í”Œë¦¬ì¼€ì´ì…˜(Frontend, Backend)ì„ ë°°í¬í•˜ê³ , ì™¸ë¶€ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ Ingressë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

4. Backend Deploymentì— HPAë¥¼ ì ìš©í•˜ì—¬ CPU/Memory ì‚¬ìš©ëŸ‰ì— ë”°ë¼ íŒŒë“œê°€ ìë™ìœ¼ë¡œ í™•ì¥/ì¶•ì†Œë˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.

5. KarpenterëŠ” ìŠ¤ì¼€ì¤„ë§ ëŒ€ê¸° ì¤‘ì¸ íŒŒë“œë¥¼ ê°ì§€í•˜ê³ , NodePool ì„¤ì •ì— ë”°ë¼ ìµœì ì˜ EC2 ì¸ìŠ¤í„´ìŠ¤ë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ì—¬ íŒŒë“œë¥¼ ë°°ì¹˜í•©ë‹ˆë‹¤.

![](https://velog.velcdn.com/images/agline/post/20b0dff3-e0cb-4334-9eb0-021346343a88/image.png)

# 1ë‹¨ê³„: Terraformìœ¼ë¡œ ì¸í”„ë¼ êµ¬ì¶•í•˜ê¸°

## VPC, subnet ê°€ìš©ì˜ì—­ a,b ë‘ê°œ
```
provider "aws" {
  region = "ap-northeast-2"
}

data "aws_availability_zones" "available" {}

locals {
  azs = slice(data.aws_availability_zones.available.names, 0, 2)
}

# VPC
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = "my-vpc"
  }
}

# ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´
resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "my-igw"
  }
}

# í¼ë¸”ë¦­ ì„œë¸Œë„·
resource "aws_subnet" "public" {
  count                   = 2
  vpc_id                  = aws_vpc.main.id
  cidr_block              = element(["10.0.1.0/24", "10.0.2.0/24"], count.index)
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name = element(["my-pub-2a", "my-pub-2b"], count.index)
  }
}

# í”„ë¼ì´ë¹— ì„œë¸Œë„·
resource "aws_subnet" "private" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = element(["10.0.101.0/24", "10.0.102.0/24"], count.index)
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = element(["my-pvt-2a", "my-pvt-2b"], count.index)
  }
}

# í¼ë¸”ë¦­ NACL
resource "aws_network_acl" "public" {
  vpc_id = aws_vpc.main.id

  egress {
    protocol   = "-1"
    rule_no    = 100
    action     = "allow"
    cidr_block = "0.0.0.0/0"
    from_port  = 0
    to_port    = 0
  }

  ingress {
    protocol   = "-1"
    rule_no    = 100
    action     = "allow"
    cidr_block = "0.0.0.0/0"
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = "my-nacl-pub"
  }
}

# í”„ë¼ì´ë¹— NACL
resource "aws_network_acl" "private" {
  vpc_id = aws_vpc.main.id

  egress {
    protocol   = "-1"
    rule_no    = 100
    action     = "allow"
    cidr_block = "0.0.0.0/0"
    from_port  = 0
    to_port    = 0
  }

  ingress {
    protocol   = "-1"
    rule_no    = 100
    action     = "allow"
    cidr_block = "0.0.0.0/0"
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = "my-nacl-pvt"
  }
}

# í¼ë¸”ë¦­ ì„œë¸Œë„·ì— í¼ë¸”ë¦­ NACL ì—°ê²°
resource "aws_network_acl_association" "public" {
  count          = 2
  subnet_id      = aws_subnet.public[count.index].id
  network_acl_id = aws_network_acl.public.id
}

# í”„ë¼ì´ë¹— ì„œë¸Œë„·ì— í”„ë¼ì´ë¹— NACL ì—°ê²°
resource "aws_network_acl_association" "private" {
  count          = 2
  subnet_id      = aws_subnet.private[count.index].id
  network_acl_id = aws_network_acl.private.id
}

# í¼ë¸”ë¦­ ë¼ìš°íŒ… í…Œì´ë¸”
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.igw.id
  }

  tags = {
    Name = "my-rtb-pub"
  }
}

# í”„ë¼ì´ë¹— ë¼ìš°íŒ… í…Œì´ë¸” (ê¸°ë³¸ ì„¤ì •)
resource "aws_route_table" "private" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "my-rtb-pvt"
  }
}

# í¼ë¸”ë¦­ ì„œë¸Œë„·ì— ë¼ìš°íŒ… í…Œì´ë¸” ì—°ê²°
resource "aws_route_table_association" "public" {
  count          = 2
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

# í”„ë¼ì´ë¹— ì„œë¸Œë„·ì— ë¼ìš°íŒ… í…Œì´ë¸” ì—°ê²°
resource "aws_route_table_association" "private" {
  count          = 2
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}

#ì¶œë ¥
output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "private_subnet_ids" {
  value = aws_subnet.private[*].id
}
```
## EKS ì„¤ì •ì„ ìœ„í•œ EIP, NAT Gateway

```
locals {
  vpc_id           = "vpc-example"
  public_subnet_ids = ["subnet-example", "subnet-example"]
  private_subnet_ids = ["subnet-example", "subnet-example"]
}

# 1. Elastic IP for NAT
resource "aws_eip" "nat_eip" {}

# 2. NAT Gateway (í¼ë¸”ë¦­ ì„œë¸Œë„·ì— ìƒì„±í•´ì•¼ í•¨)
resource "aws_nat_gateway" "nat" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = "subnet-example"  # í¼ë¸”ë¦­ ì„œë¸Œë„· ì¤‘ í•˜ë‚˜
  tags = {
    Name = "my-nat-gw"
  }
}

# 3. í”„ë¼ì´ë¹— ë¼ìš°íŒ… í…Œì´ë¸” NAT Gateway ë¼ìš°íŒ…
resource "aws_route" "private" {
  route_table_id       = "rtb-example"
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id       = aws_nat_gateway.nat.id
}
```
## EKSì ‘ê·¼ì„ ìœ„í•œ Bastion EC2
```
provider "aws" {
  region = "ap-northeast-2"

  access_key = var.aws_access_key_id
  secret_key = var.aws_secret_access_key
}

##########################
# Bastion ë³´ì•ˆ ê·¸ë£¹
##########################

resource "aws_security_group" "bastion_sg" {
  name        = "bastion-sg"
  description = "Allow SSH"
  vpc_id      = local.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "bastion-sg"
  }
}

resource "aws_iam_role" "bastion_role" {
  name = "bastionRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Service = "ec2.amazonaws.com"
      },
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "bastion_policy" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
    "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
    "arn:aws:iam::aws:policy/AdministratorAccess"
  ])
  role       = aws_iam_role.bastion_role.name
  policy_arn = each.value
}

resource "aws_iam_instance_profile" "bastion_profile" {
  name = "bastionInstanceProfile"
  role = aws_iam_role.bastion_role.name
}

##########################
# Bastion EC2 ì¸ìŠ¤í„´ìŠ¤
##########################

resource "aws_instance" "bastion" {
  ami                   = "ami-0fc8aeaa301af7663"
  instance_type         = "t3.micro"
  subnet_id             = local.public_subnet_ids[0]
  key_name              = "bastion-key"
  vpc_security_group_ids = [aws_security_group.bastion_sg.id]
  associate_public_ip_address = true
  iam_instance_profile        = aws_iam_instance_profile.bastion_profile.name

  user_data = <<-EOF
              #!/bin/bash
              yum update -y
              yum install -y unzip curl wget
              # AWS CLI v2 ì„¤ì¹˜
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
              unzip awscliv2.zip
              ./aws/install
              # MariaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜
              yum install -y mariadb105
              # git ì„¤ì¹˜
              sudo yum install -y git
              # kubectl ì„¤ì¹˜
              curl -LO https://dl.k8s.io/release/v1.33.0/bin/linux/amd64/kubectl
              chmod +x kubectl
              mv kubectl /usr/local/bin/
              # k9s ì„¤ì¹˜
              K9S_URL=$(curl -s https://api.github.com/repos/derailed/k9s/releases/latest \
                | grep "browser_download_url.*Linux_amd64.tar.gz" \
                | cut -d '"' -f 4 \
                | head -n 1)
              wget "$K9S_URL" -O k9s_Linux_amd64.tar.gz
              tar -xvf k9s_Linux_amd64.tar.gz
              mv k9s /usr/local/bin/
              rm -f k9s_Linux_amd64.tar.gz
              # AWS CLI ê¸°ë³¸ ì„¤ì •
              mkdir -p /home/ec2-user/.aws
              cat > /home/ec2-user/.aws/credentials <<EOL
              [default]
              aws_access_key_id = ${var.aws_access_key_id}
              aws_secret_access_key = ${var.aws_secret_access_key}
              EOL
              cat > /home/ec2-user/.aws/config <<EOL
              [default]
              region = ap-northeast-2
              output = json
              EOL
              chown -R ec2-user:ec2-user /home/ec2-user/.aws
              EOF

  tags = {
    Name = "bastion"
  }
}
```
## RDS
```
##########################
# RDS ë³´ì•ˆ ê·¸ë£¹ (MariaDBìš©)
##########################

resource "aws_security_group" "rds_sg" {
  name   = "rds-sg"
  vpc_id = local.vpc_id

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = "tcp"
    security_groups = [aws_security_group.bastion_sg.id]  
    description 	= Bastionì—ì„œ ì ‘ê·¼ í—ˆìš©
  }

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = "tcp"
    security_groups = [aws_eks_cluster.main.vpc_config[0].cluster_security_group_id]
    description     = EKS í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ í—ˆìš©
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "rds-sg"
  }
}

##########################
# RDS ì„œë¸Œë„· ê·¸ë£¹
##########################

resource "aws_db_subnet_group" "mariadb_subnet_group" {
  name       = "mariadb-subnet-group"
  subnet_ids = local.private_subnet_ids

  tags = {
    Name = "mariadb-subnet-group"
  }
}

##########################
# RDS ì¸ìŠ¤í„´ìŠ¤ (MariaDB)
##########################

resource "aws_db_instance" "mariadb" {
  identifier              = "mariadb-instance"
  allocated_storage       = 20
  engine                  = "mariadb"
  engine_version          = "10.6"
  instance_class          = "db.t3.small"
  db_name                 = "mydb"
  username                = "admin"
  password                = "MySecurePass123!"  # í…ŒìŠ¤íŠ¸ìš©. ìš´ì˜ ì‹œ Secrets Manager ì‚¬ìš© ê¶Œì¥
  skip_final_snapshot     = true
  publicly_accessible     = false
  vpc_security_group_ids  = [aws_security_group.rds_sg.id]
  db_subnet_group_name    = aws_db_subnet_group.mariadb_subnet_group.name

  tags = {
    Name = "mariadb"
  }
}
```
## EKS cluster, NodeGroup ìƒì„±
```
##########################
# EKS í´ëŸ¬ìŠ¤í„° IAM Role
##########################

resource "aws_iam_role" "eks_role" {
  name = "eksClusterRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = {
        Service = "eks.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "eks_attach" {
  role       = aws_iam_role.eks_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

##########################
# EKS í´ëŸ¬ìŠ¤í„°
##########################

resource "aws_eks_cluster" "main" {
  name     = "my-eks-cluster"
  role_arn = aws_iam_role.eks_role.arn

  vpc_config {
    subnet_ids = local.private_subnet_ids
  }

  depends_on = [aws_iam_role_policy_attachment.eks_attach]
}

##########################
# EKS ë…¸ë“œ ê·¸ë£¹ IAM Role & Instance Profile
##########################

resource "aws_iam_role" "eks_node_role" {
  name = "eksNodeGroupRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Service = "ec2.amazonaws.com"
      },
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "eks_node_policy" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
    "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  ])

  role       = aws_iam_role.eks_node_role.name
  policy_arn = each.value
}

# Karpenterì—ì„œ ì°¸ì¡°í•  EKS ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±
resource "aws_iam_instance_profile" "eks_node_profile" {
  name = "eksNodeGroupProfile"
  role = aws_iam_role.eks_node_role.name
}

##########################
# EKS Managed Node Group
##########################

resource "aws_eks_node_group" "node_group" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "my-node-group"
  node_role_arn   = aws_iam_role.eks_node_role.arn
  subnet_ids      = local.private_subnet_ids

  scaling_config {
    desired_size = 1
    max_size     = 3
    min_size     = 1
  }

  instance_types = ["t3.small"]

  disk_size = 20

  depends_on = [
    aws_iam_role_policy_attachment.eks_node_policy,
    aws_eks_cluster.main
  ]

  tags = {
    Name = "eks-node-group"
  }
}
```
# 2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ì— ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜í•˜ê¸°
## AWS Load Balancer Controller (LBC)
https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/main/docs/install/iam_policy.json
```
##########################
# LBC IAM
##########################
data "aws_eks_cluster" "cluster" {
  name = aws_eks_cluster.main.name
}

data "aws_eks_cluster_auth" "cluster" {
  name = aws_eks_cluster.main.name
}

data "tls_certificate" "eks_cluster" {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "oidc_provider" {
  url           = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.eks_cluster.certificates[0].sha1_fingerprint]
}

resource "aws_iam_policy" "lbc_policy" {
  name        = "AWSLoadBalancerControllerIAMPolicy"
  description = "Policy for AWS Load Balancer Controller"
  policy      = file("iam_policy_lbc.json")
}

resource "aws_iam_role" "lbc_sa_role" {
  name = "lbc-sa-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Federated = aws_iam_openid_connect_provider.oidc_provider.arn },
      Action    = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, "https://", "")}:sub" = "system:serviceaccount:kube-system:aws-load-balancer-controller"
        }
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "lbc_attach" {
  role       = aws_iam_role.lbc_sa_role.name
  policy_arn = aws_iam_policy.lbc_policy.arn
}

##########################
# LBC ì„¤ì¹˜
##########################
resource "null_resource" "install_lbc" {
  depends_on = [
    aws_instance.bastion,
    aws_eks_cluster.main,
    aws_eks_node_group.node_group,
    aws_iam_role_policy_attachment.lbc_attach
  ]

  connection {
    type        = "ssh"
    host        = aws_instance.bastion.public_ip
    user        = "ec2-user"
    private_key = file("C:/.ssh/bastion-key.pem")
    timeout     = "10m"
  }

  provisioner "remote-exec" {
    inline = [
      "until aws eks --region ap-northeast-2 describe-cluster --name my-eks-cluster --query cluster.status --output text | grep -q 'ACTIVE'; do echo 'Waiting for EKS cluster to become active...'; sleep 30; done",
      "aws eks update-kubeconfig --region ap-northeast-2 --name my-eks-cluster",
      "curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash",
      "helm repo add eks https://aws.github.io/eks-charts",
      "helm repo update",

      "kubectl create ns kube-system || true",

      # ê¸°ì¡´ ì„¤ì¹˜ ì™„ì „íˆ ì •ë¦¬
      "echo 'Cleaning up existing installation...'",
      "helm uninstall aws-load-balancer-controller -n kube-system || true",
      "kubectl delete deployment aws-load-balancer-controller -n kube-system || true",
      "kubectl delete serviceaccount aws-load-balancer-controller -n kube-system || true",
      "kubectl delete secrets -l name=aws-load-balancer-controller -n kube-system || true",

      <<-EOT
      cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: aws-load-balancer-controller
        namespace: kube-system
        annotations:
          eks.amazonaws.com/role-arn: ${aws_iam_role.lbc_sa_role.arn}
        labels:
          app.kubernetes.io/name: aws-load-balancer-controller
          app.kubernetes.io/component: controller
      EOF
      EOT
      ,
      
      # Helmìœ¼ë¡œ ì„¤ì¹˜ (ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸ëŠ” ìƒì„±í•˜ì§€ ì•Šê³  ê¸°ì¡´ ê²ƒ ì‚¬ìš©)
      "helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=my-eks-cluster --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller --set region=ap-northeast-2 --set vpcId=${local.vpc_id}"
    ]
  }
}
```
## Metrics Server
```
############################################
# Metrics Server Helm ì„¤ì¹˜
############################################
resource "null_resource" "install_metrics_server" {
  depends_on = [
    null_resource.install_karpenter
  ]

  connection {
    type        = "ssh"
    host        = aws_instance.bastion.public_ip
    user        = "ec2-user"
    private_key = file("C:/.ssh/bastion-key.pem")
  }

  provisioner "remote-exec" {
    inline = [
      "helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/",
      "helm repo update",
      "helm upgrade --install metrics-server metrics-server/metrics-server --namespace kube-system",
      "echo 'Metrics Server installation complete.'"
    ]
  }
}
```
## Karpenter

https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/#create-the-karpentercontroller-iam-role
```
############################################
# Karpenter IAM Role & Policy
############################################
resource "aws_iam_role" "karpenter_controller_role" {
  name = "KarpenterControllerRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Federated = aws_iam_openid_connect_provider.oidc_provider.arn
      },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, "https://", "")}:sub" = "system:serviceaccount:karpenter:karpenter"
        }
      }
    }]
  })
}

data "aws_caller_identity" "current" {}

resource "aws_iam_policy" "karpenter_controller_policy" {
  name   = "KarpenterControllerPolicy"
  policy = templatefile("karpenter-controller-policy.json", {
    AWS_PARTITION  = "aws"
    AWS_ACCOUNT_ID = data.aws_caller_identity.current.account_id
    AWS_REGION     = "ap-northeast-2"
    CLUSTER_NAME   = aws_eks_cluster.main.name
  })
}

resource "aws_iam_role_policy_attachment" "karpenter_controller_attach" {
  role       = aws_iam_role.karpenter_controller_role.name
  policy_arn = aws_iam_policy.karpenter_controller_policy.arn
}

############################################
# ì„œë¸Œë„·ê³¼ ë³´ì•ˆê·¸ë£¹ì— Karpenter íƒœê·¸ ì¶”ê°€
############################################
resource "aws_ec2_tag" "private_subnet_tags" {
  count       = length(local.private_subnet_ids)
  resource_id = local.private_subnet_ids[count.index]
  key         = "karpenter.sh/discovery"
  value       = aws_eks_cluster.main.name
}

# EKS í´ëŸ¬ìŠ¤í„°ì˜ ë³´ì•ˆê·¸ë£¹ì— íƒœê·¸ ì¶”ê°€
resource "aws_ec2_tag" "cluster_security_group_tag" {
  resource_id = aws_eks_cluster.main.vpc_config[0].cluster_security_group_id
  key         = "karpenter.sh/discovery"
  value       = aws_eks_cluster.main.name
}

############################################
# Karpenter Helm ì„¤ì¹˜
############################################
resource "null_resource" "install_karpenter" {
  depends_on = [
    aws_instance.bastion,
    aws_eks_cluster.main,
    aws_eks_node_group.node_group,
    null_resource.install_lbc,
    aws_ec2_tag.private_subnet_tags,
    aws_ec2_tag.cluster_security_group_tag
  ]

  connection {
    type        = "ssh"
    host        = aws_instance.bastion.public_ip
    user        = "ec2-user"
    private_key = file("C:/.ssh/bastion-key.pem")
  }

  provisioner "remote-exec" {
    inline = [
      <<EOT
      # LBC deploymentê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
      kubectl wait --namespace=kube-system deployment/aws-load-balancer-controller --for=condition=Available=True --timeout=5m
      
      helm repo add karpenter https://charts.karpenter.sh/
      helm repo update
      kubectl create namespace karpenter || true
      
      # Karpenter v1.6.0 ì„¤ì¹˜
      helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.6.0 \
        --namespace karpenter \
        --create-namespace \
        --set "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn=${aws_iam_role.karpenter_controller_role.arn}" \
        --set "settings.clusterName=${aws_eks_cluster.main.name}" \
        --set "settings.clusterEndpoint=${aws_eks_cluster.main.endpoint}" \
        --set "settings.defaultInstanceProfile=${aws_iam_instance_profile.eks_node_profile.name}" \
        --set "tolerations[0].key=karpenter.sh/unschedulable" \
        --set "tolerations[0].operator=Exists" \
        --set "tolerations[0].effect=NoSchedule" \
        --set "replicas=1" \
        --set "topologySpreadConstraints[0].maxSkew=2" \
        --set "topologySpreadConstraints[0].topologyKey=topology.kubernetes.io/zone" \
        --set "topologySpreadConstraints[0].whenUnsatisfiable=ScheduleAnyway" \
        --set "topologySpreadConstraints[0].labelSelector.matchLabels.app\\.kubernetes\\.io/name=karpenter"
      EOT
    ]
  }
}
```
### 1. Karpenter IAM Role & Policy
```
resource "aws_iam_role" "karpenter_controller_role" {
  name = "KarpenterControllerRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Federated = aws_iam_openid_connect_provider.oidc_provider.arn
      },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, "https://", "")}:sub" = "system:serviceaccount:karpenter:karpenter"
        }
      }
    }]
  })
}
```
- Principal: Federated = aws_iam_openid_connect_provider.oidc_provider.arn
	- EKS í´ëŸ¬ìŠ¤í„°ì˜ OIDC Identity Providerë¥¼ í†µí•´ ì¸ì¦

- Action: sts:AssumeRoleWithWebIdentity
	- Kubernetes ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸ê°€ AWS IAM ì—­í• ì„ assumeí•  ìˆ˜ ìˆê²Œ í•¨
- Condition: StringEquals
	- íŠ¹ì • ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸(system:serviceaccount:karpenter:karpenter)ë§Œ ì´ ì—­í• ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì œí•œ

#### EKSì—ì„œ OIDCë¥¼ ì™œ ì‚¬ìš©í• ê¹Œ? 
**OIDC (OpenID Connect)**ëŠ” OAuth 2.0 ìœ„ì— êµ¬ì¶•ëœ ì‹ ì› ì¸ì¦ í”„ë¡œí† ì½œì…ë‹ˆë‹¤.

> ğŸ¤” Kubernetes Podì´ AWS ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´?
   â†’ AWS ì•¡ì„¸ìŠ¤ í‚¤ê°€ í•„ìš”
   â†’ í•˜ì§€ë§Œ Podì— í‚¤ë¥¼ í•˜ë“œì½”ë”©í•˜ëŠ” ê²ƒì€ ë³´ì•ˆìƒ ìœ„í—˜!
   âœ… Pod â†’ Kubernetes Service Account â†’ AWS IAM Role
   ì•ˆì „í•˜ê³  ìë™í™”ëœ ë°©ì‹ìœ¼ë¡œ AWS ê¶Œí•œ íšë“!
   

```
data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer

ì‹¤ì œ ê°’ ì˜ˆì‹œ
https://oidc.eks.ap-northeast-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E

URLì—ì„œ "https://" ì œê±°
replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, "https://", ""):sub

ê²°ê³¼
oidc.eks.ap-northeast-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E:sub

ì‹¤ì œ ì™„ì„±ëœ ì¡°ê±´ë¬¸
{
  "oidc.eks.ap-northeast-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E:sub": "system:serviceaccount:karpenter:karpenter"
}
```

>ğŸ”’ ì´ ì¡°ê±´ë¬¸ì˜ ë³´ì•ˆ íš¨ê³¼:
âŒ ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ Service Account â†’ ì ‘ê·¼ ê±°ë¶€
âŒ ë‹¤ë¥¸ ì´ë¦„ì˜ Service Account â†’ ì ‘ê·¼ ê±°ë¶€  
âŒ ì¼ë°˜ ì‚¬ìš©ìë‚˜ ë‹¤ë¥¸ ì¸ì¦ ë°©ì‹ â†’ ì ‘ê·¼ ê±°ë¶€
âœ… karpenter ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ karpenter Service Accountë§Œ â†’ ì ‘ê·¼ í—ˆìš©

#### AWS Account ID ê°€ì ¸ì˜¤ê¸°
```
data "aws_caller_identity" "current" {}
```
í˜„ì¬ AWS ê³„ì •ì˜ IDë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì™€ ì •ì±…ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### Karpenter Controller Policy
```
resource "aws_iam_policy" "karpenter_controller_policy" {
  name   = "KarpenterControllerPolicy"
  policy = templatefile("karpenter-controller-policy.json", {
    AWS_PARTITION  = "aws"
    AWS_ACCOUNT_ID = data.aws_caller_identity.current.account_id
    AWS_REGION     = "ap-northeast-2"
    CLUSTER_NAME   = aws_eks_cluster.main.name
  })
}
```
**templatefile í•¨ìˆ˜**:
- ì™¸ë¶€ JSON íŒŒì¼(karpenter-controller-policy.json)ì„ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©
ë³€ìˆ˜ë“¤ì„ ë™ì ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ì •ì±… ìƒì„±

**ì£¼ìš” ê¶Œí•œ (ì¼ë°˜ì ìœ¼ë¡œ í¬í•¨ë˜ëŠ” ê²ƒë“¤)**:
- EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±/ì‚­ì œ/ìˆ˜ì •
- Auto Scaling Group ê´€ë¦¬
- IAM ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ ì—°ê²°
- íƒœê·¸ ê´€ë¦¬
- EKS ë…¸ë“œ ê·¸ë£¹ ê´€ë¦¬

### 2. ì„œë¸Œë„·ê³¼ ë³´ì•ˆê·¸ë£¹ì— Karpenter íƒœê·¸ ì¶”ê°€
```
resource "aws_ec2_tag" "private_subnet_tags" {
  count       = length(local.private_subnet_ids)
  resource_id = local.private_subnet_ids[count.index]
  key         = "karpenter.sh/discovery"
  value       = aws_eks_cluster.main.name
}
```
Karpenterê°€ ë…¸ë“œë¥¼ ë°°í¬í•  ì„œë¸Œë„·ì„ í”„ë¼ì´ë¹— ì„œë¸Œë„·ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ íƒœê·¸ ì¶”ê°€
KarpenterëŠ” ì´ íƒœê·¸ë¥¼ í†µí•´ ì–´ëŠ ì„œë¸Œë„·ì— ìƒˆ ë…¸ë“œë¥¼ ìƒì„±í• ì§€ ê²°ì •
### 3. karpneter helm ì„¤ì¹˜
```
Karpenter v1.6.0 ì„¤ì¹˜
helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.6.0 \
--namespace karpenter \
--create-namespace \
--set "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn=${aws_iam_role.karpenter_controller_role.arn}" \
--set "settings.clusterName=${aws_eks_cluster.main.name}" \
--set "settings.clusterEndpoint=${aws_eks_cluster.main.endpoint}" \
--set "settings.defaultInstanceProfile=${aws_iam_instance_profile.eks_node_profile.name}" \
--set "tolerations[0].key=karpenter.sh/unschedulable" \ â†’ "êµì²´ ì˜ˆì • ë…¸ë“œì—ì„œë„ ì‹¤í–‰í•  ìˆ˜ ìˆì–´"
--set "tolerations[0].operator=Exists" \ â†’ "ê°’ì´ ë­ë“  ìƒê´€ì—†ì–´, ìœ ì—°í•˜ê²Œ ëŒ€ì‘í• ê²Œ"
--set "tolerations[0].effect=NoSchedule" \ â†’ "ìŠ¤ì¼€ì¤„ë§ ì œí•œì€ ê´œì°®ì§€ë§Œ ê°•ì œ ì¶•ì¶œì€ ì‹«ì–´"
--set "replicas=1" \ â†’ "ë‚˜ í˜¼ìì„œë„ ì¶©ë¶„í•´, ë¦¬ì†ŒìŠ¤ ì•„ë¼ì"
--set "topologySpreadConstraints[0].maxSkew=2" \ â†’ "ì˜ì—­ ê°„ ì°¨ì´ê°€ 2ê°œ ì´í•˜ë©´ ê´œì°®ì•„"
--set "topologySpreadConstraints[0].topologyKey=topology.kubernetes.io/zone" \ â†’ "ê°€ìš©ì˜ì—­ë³„ë¡œ ë¶„ì‚°í•´ì¤˜"
--set "topologySpreadConstraints[0].whenUnsatisfiable=ScheduleAnyway" \ â†’ "ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ì¼ë‹¨ ì‹¤í–‰í•˜ëŠ” ê²Œ ì¤‘ìš”í•´"
--set "topologySpreadConstraints[0].labelSelector.matchLabels.app\\.kubernetes\\.io/name=karpenter" â†’ "ì´ ì •ì±…ì€ ë‚˜(Karpenter)ì—ê²Œë§Œ ì ìš©í•´ì¤˜"
```
#### tolerations[0].key=karpenter.sh/unschedulable
```
ì‹¤ì œìƒí™©ì˜ˆì‹œ:

ìƒí™©: ë…¸ë“œ Aê°€ êµì²´ ì˜ˆì • ìƒíƒœ
  â†“
ê¸°ì¡´ Podë“¤: "ì´ ë…¸ë“œ ìœ„í—˜í•´, ë‹¤ë¥¸ ê³³ìœ¼ë¡œ í”¼í•˜ì!" ğŸƒâ€â™‚ï¸
  â†“  
Karpenter: "ì–´? ê·¸ëŸ¼ ë‚´ê°€ ëˆ„ê°€ ìƒˆ ë…¸ë“œ ë§Œë“¤ì–´ì¤˜?" ğŸ¤”
  â†“
Karpenterê°€ í”¼í•´ë²„ë¦¬ë©´: ìƒˆ ë…¸ë“œ ìƒì„± ë¶ˆê°€! ğŸ˜±

ì™œ í•„ìš”í•œê°€?
ì¼ë°˜ ì•± Pod: êµì²´ ì˜ˆì • ë…¸ë“œ í”¼í•¨ (ë°ì´í„° ë³´í˜¸)
Karpenter: êµì²´ ì˜ˆì • ë…¸ë“œì—ì„œë„ ì‹¤í–‰ (ì„œë¹„ìŠ¤ ì—°ì†ì„±)

```
#### tolerations[0].operator=Exists
```
ì‹¤ì œìƒí™©ì˜ˆì‹œ:

ì‹œë‚˜ë¦¬ì˜¤ 1: Taint ê°’ì´ "replacing"
ë…¸ë“œ ìƒíƒœ: karpenter.sh/unschedulable=replacing

ì‹œë‚˜ë¦¬ì˜¤ 2: Taint ê°’ì´ "draining"  
ë…¸ë“œ ìƒíƒœ: karpenter.sh/unschedulable=draining

ì‹œë‚˜ë¦¬ì˜¤ 3: Taint ê°’ì´ "upgrading"
ë…¸ë“œ ìƒíƒœ: karpenter.sh/unschedulable=upgrading

ë§Œì•½ íŠ¹ì • ê°’ë§Œ í—ˆìš©í•œë‹¤ë©´?
ì´ë ‡ê²Œ ì„¤ì •í–ˆë‹¤ë©´:
tolerations:
- key: karpenter.sh/unschedulable
  value: "replacing"  # íŠ¹ì • ê°’ë§Œ!
  operator: Equal
  
ë¬¸ì œ ë°œìƒ:
- "draining" ìƒíƒœì—ì„œëŠ” ì‹¤í–‰ ë¶ˆê°€ âŒ
- "upgrading" ìƒíƒœì—ì„œë„ ì‹¤í–‰ ë¶ˆê°€ âŒ
â†’ Karpenter ì„œë¹„ìŠ¤ ì¤‘ë‹¨! ğŸ˜±

Existsì˜ ì¥ì :
operator: Exists = "í‚¤ë§Œ ìˆìœ¼ë©´ ê°’ì€ ë­ë“  OK!"
â†’ ëª¨ë“  êµì²´ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€ì‘ ê°€ëŠ¥ âœ…
```
#### tolerations[0].effect=NoSchedule
```
NoSchedule vs NoExecute ì°¨ì´:
- NoSchedule: "ìƒˆë¡œ ìŠ¤ì¼€ì¤„ë§ ì•ˆ í•´ì¤„ë˜" (ê¸°ì¡´ ê²ƒì€ ê³„ì† ì‹¤í–‰)
- NoExecute:  "ê¸°ì¡´ ê²ƒë„ ê°•ì œë¡œ ì«“ì•„ë‚¼ë˜" (ì¦‰ì‹œ ì¢…ë£Œ)

ì‹¤ì œìƒí™©ì˜ˆì‹œ:
ìƒí™©: ë…¸ë“œì— NoExecute Taint ì ìš©ë¨
  â†“
Karpenter Pod: ì¦‰ì‹œ ê°•ì œ ì¢…ë£Œ ğŸ˜µ
  â†“
ìƒˆë¡œìš´ ìŠ¤ì¼€ì¼ë§ ìš”ì²­: "Karpenter ì–´ë””ê°”ì–´?" ğŸ¤·â€â™‚ï¸
  â†“  
ê²°ê³¼: í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§ ë§ˆë¹„! ğŸ˜±

ì™œ NoScheduleë§Œ?
NoSchedule Toleration:
- ê¸°ì¡´ Karpenter Pod: ê³„ì† ì‹¤í–‰ âœ…
- ìƒˆ Karpenter Pod: ë‹¤ë¥¸ ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§ âœ…
- ì„œë¹„ìŠ¤ ì—°ì†ì„±: ë³´ì¥ë¨ âœ…
```
#### replicas=1
```
Karpenterì˜ ì£¼ìš” ì—…ë¬´:
1. í´ëŸ¬ìŠ¤í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§ 
2. ìŠ¤ì¼€ì¼ë§ ê²°ì •  
3. AWS API í˜¸ì¶œ 
4. ë…¸ë“œ ìƒì„±/ì‚­ì œ ì§€ì‹œ 

ì™œ í•œê°œë©´ ì¶©ë¶„í• ê¹Œ?
âŒ ì—¬ëŸ¬ ê°œê°€ ë™ì‹œì— ê°™ì€ ì¼ì„ í•˜ë©´:
   Pod A: "ë…¸ë“œ 3ê°œ ë” í•„ìš”í•´!"
   Pod B: "ë…¸ë“œ 3ê°œ ë” í•„ìš”í•´!"  
   ê²°ê³¼: 6ê°œ ìƒì„± â†’ ìì› ë‚­ë¹„ ğŸ˜±

âœ… 1ê°œê°€ ëª¨ë“  ê±¸ ê´€ë¦¬í•˜ë©´:
   ë‹¨ì¼ ê²°ì •ì: "ë…¸ë“œ 3ê°œë§Œ ì¶”ê°€í•˜ì"
   ê²°ê³¼: ì •í™•íˆ 3ê°œ ìƒì„± â†’ íš¨ìœ¨ì !

Karpenterì˜ ê³ ê°€ìš©ì„±ì€?
í•œê°œë¡œ ìš´ì˜ì¤‘ì¸ Karpenter Pod ì£½ìœ¼ë©´?
â†’ Kubernetes Deploymentê°€ ìë™ìœ¼ë¡œ ì¬ì‹œì‘
â†’ ì ê¹ ì¤‘ë‹¨ë˜ì§€ë§Œ ê³§ ë³µêµ¬ âœ…

vs ì—¬ëŸ¬ ê°œ Karpenter Pod ìš´ì˜ ì‹œ:
â†’ ë³µì¡í•œ ë¦¬ë” ì„ ì¶œ í•„ìš”
â†’ ë™ì‹œì„± ì œì–´ ë³µì¡
â†’ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
```
#### maxSkew=2
```
ì‹¤ì œ ê°€ìš©ì˜ì—­ ì‹œë‚˜ë¦¬ì˜¤:
í˜„ì¬ ìƒí™©:
- AZ-A: Karpenter Pod 0ê°œ
- AZ-B: Karpenter Pod 1ê°œ  
- AZ-C: Karpenter Pod 0ê°œ

ì°¨ì´: ìµœëŒ€ 1ê°œ (1-0=1) â†’ maxSkew=2 ì¡°ê±´ ë§Œì¡± âœ…

ì™œ 2ê°œê¹Œì§€ í—ˆìš©?
ë„ˆë¬´ ì—„ê²©í•œ ê²½ìš° (maxSkew=0):
- AZ-A: 0ê°œ, AZ-B: 1ê°œ â†’ ì°¨ì´ 1ê°œ âŒ
- ìƒˆ Podì„ AZ-Aë‚˜ AZ-Cì— ê°•ì œë¡œ ë°°ì¹˜
- í•˜ì§€ë§Œ ê·¸ ì˜ì—­ì— ì ì ˆí•œ ë…¸ë“œê°€ ì—†ì„ ìˆ˜ë„...

ì ë‹¹íˆ ìœ ì—°í•œ ê²½ìš° (maxSkew=2):
- AZ-A: 0ê°œ, AZ-B: 2ê°œ, AZ-C: 0ê°œ â†’ ì°¨ì´ 2ê°œ âœ…  
- ì—¬ì „íˆ ë¶„ì‚°ë˜ì–´ ìˆê³ , ìŠ¤ì¼€ì¤„ë§ë„ ìœ ì—°í•¨

ì‹¤ì œ ì¥ì•  ìƒí™©:
AZ-A ì¥ì•  ë°œìƒ! ğŸ”¥
- maxSkew=0: Podì´ AZ-Aì— ì—†ì–´ì„œ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ âŒ
- maxSkew=2: AZ-Bë‚˜ AZ-Cì— ì—¬ìœ ë¡­ê²Œ ë°°ì¹˜ âœ…
```
#### whenUnsatisfiable=ScheduleAnyway
```
ì™„ë²½í•œ ë¶„ì‚°ì´ ë¶ˆê°€ëŠ¥í•œ ìƒí™©ë“¤:

ìƒí™© 1: ë…¸ë“œ ë¶€ì¡±
í´ëŸ¬ìŠ¤í„° êµ¬ì„±:
- AZ-A: ë…¸ë“œ ìˆìŒ âœ…
- AZ-B: ë…¸ë“œ ì—†ìŒ âŒ  
- AZ-C: ë…¸ë“œ ì—†ìŒ âŒ

ì™„ë²½í•œ ë¶„ì‚° ìš”êµ¬ ì‹œ: ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ âŒ
ìœ ì—°í•œ ì •ì±…: AZ-Aì—ë¼ë„ ë°°ì¹˜ âœ…

ìƒí™© 2: ë¦¬ì†ŒìŠ¤ ë¶€ì¡±
ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­:
- Karpenter Pod: CPU 500m, Memory 512Mi í•„ìš”

í˜„ì¬ ìƒí™©:
- AZ-A: ë¦¬ì†ŒìŠ¤ ì¶©ë¶„ âœ…
- AZ-B: ë¦¬ì†ŒìŠ¤ ë¶€ì¡± âŒ
- AZ-C: ë¦¬ì†ŒìŠ¤ ë¶€ì¡± âŒ

ê²°ê³¼: AZ-Aì— ë°°ì¹˜ë˜ì–´ ì„œë¹„ìŠ¤ ê³„ì† ì œê³µ âœ…

DoNotSchedule vs ScheduleAnyway:
DoNotSchedule: "ì™„ë²½í•˜ì§€ ì•Šìœ¼ë©´ ì•„ì˜ˆ ì‹¤í–‰ ì•ˆ í•´!"
â†’ Karpenter ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ê°€ëŠ¥ ğŸ˜±

ScheduleAnyway: "ë¶ˆì™„ì „í•´ë„ ì¼ë‹¨ ì‹¤í–‰í•´ì„œ ì„œë¹„ìŠ¤ ìœ ì§€!"  
â†’ ê³ ê°€ìš©ì„± ìš°ì„  âœ…
```
#### labelSelector.matchLabels.app\\.kubernetes\\.io/name=karpenter
```
"ë‹¤ë¥¸ Podë“¤ì´ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆë„ë¡ ë…¸ë“œë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒ"

ë”°ë¼ì„œ: 
1. ë‹¤ë¥¸ Podë“¤ì´ í”¼í•˜ëŠ” ë…¸ë“œì—ì„œë„ â†’ ë‚˜ëŠ” ì‹¤í–‰ë˜ì–´ì•¼ í•¨
2. ì–´ë–¤ ìƒí™©ì—ì„œë“  â†’ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•´ì•¼ í•¨  
3. ê°•ì œ ì¢…ë£Œë˜ë©´ ì•ˆ ë˜ê³  â†’ ì„œë¹„ìŠ¤ ì—°ì†ì„± ìœ ì§€
4. íš¨ìœ¨ì ìœ¼ë¡œ â†’ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ê¸ˆì§€
5. ê°€ìš©ì„±ì„ ìœ„í•´ â†’ ì ë‹¹í•œ ë¶„ì‚°
6. ì™„ë²½í•˜ì§€ ì•Šì•„ë„ â†’ ì„œë¹„ìŠ¤ê°€ ë¨¼ì €
```

#### ê²°êµ­ ì´ ëª¨ë“  ì„¤ì •ì€ "Karpenterê°€ ì–´ë–¤ ìƒí™©ì—ì„œë“  ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´, í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ì•ˆì •ì„±ê³¼ ë¹„ìš© íš¨ìœ¨ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•¨"ì…ë‹ˆë‹¤! 
# 3ë‹¨ê³„: Karpenter NodePool & EC2NodeClass
```
############################################
# Karpenter v1.6.0 NodePool & EC2NodeClass ìƒì„±
############################################
resource "null_resource" "karpenter_nodepool" {
  depends_on = [null_resource.install_karpenter]
  
  connection {
    type        = "ssh"
    host        = aws_instance.bastion.public_ip
    user        = "ec2-user"
    private_key = file("C:/.ssh/bastion-key.pem")
  }
  
  provisioner "remote-exec" {
    inline = [
      <<-EOT
      echo "Waiting for Karpenter controller to become available..."
      kubectl wait --namespace=karpenter deployment/karpenter --for=condition=Available=True --timeout=10m

      echo "Waiting for Karpenter CRDs to be established..."
      # CRDê°€ ë“±ë¡ë  ë•Œê¹Œì§€ ëŒ€ê¸°
      kubectl wait --for condition=established --timeout=300s crd/nodepools.karpenter.sh
      kubectl wait --for condition=established --timeout=300s crd/ec2nodeclasses.karpenter.k8s.aws

      # CRD ìƒíƒœ í™•ì¸
      echo "Checking CRD status..."
      kubectl get crd | grep karpenter

      # API ë²„ì „ í™•ì¸
      echo "Checking available API versions..."
      kubectl api-resources | grep -E "(nodepool|ec2nodeclass)"

      # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„
      sleep 30

      # EC2NodeClass ìƒì„± (v1 API)
      echo "Creating EC2NodeClass..."
      cat <<EOF | kubectl apply -f -
      apiVersion: karpenter.k8s.aws/v1
      kind: EC2NodeClass
      metadata:
        name: default
      spec:
        # AMI ì„¤ì •
        amiSelectorTerms:
          - alias: al2023@latest # Karpenterê°€ ì œê³µí•˜ëŠ” AL2023 AMI ì‚¬ìš©
        
        # ì„œë¸Œë„· ì„ íƒ
        subnetSelectorTerms:
          - tags:
              karpenter.sh/discovery: "${aws_eks_cluster.main.name}"
        
        # ë³´ì•ˆ ê·¸ë£¹ ì„ íƒ
        securityGroupSelectorTerms:
          - tags:
              karpenter.sh/discovery: "${aws_eks_cluster.main.name}"
        
        # IAM ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼
        instanceProfile: "${aws_iam_instance_profile.eks_node_profile.name}"
        
        # ì‚¬ìš©ì ë°ì´í„° ìŠ¤í¬ë¦½íŠ¸
        userData: |
          #!/bin/bash
          /etc/eks/bootstrap.sh ${aws_eks_cluster.main.name}
          
          # ì¶”ê°€ ì„¤ì •
          echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
          sysctl -p /etc/sysctl.conf
        
        # ë¸”ë¡ ë””ë°”ì´ìŠ¤ ë§¤í•‘
        blockDeviceMappings:
          - deviceName: /dev/xvda
            ebs:
              volumeSize: 20Gi
              volumeType: gp3
              deleteOnTermination: true
        
        # ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤ ì„¤ì •
        metadataOptions:
          httpEndpoint: enabled
          httpProtocolIPv6: disabled
          httpPutResponseHopLimit: 2
          httpTokens: required
      EOF

      # EC2NodeClass ìƒì„± í™•ì¸
      if kubectl get ec2nodeclass default; then
        echo "EC2NodeClass created successfully"
      else
        echo "Failed to create EC2NodeClass"
        exit 1
      fi

      # ì ì‹œ ëŒ€ê¸°
      sleep 10

      # NodePool ìƒì„± (v1 API)
      echo "Creating NodePool..."
      cat <<EOF | kubectl apply -f -
      apiVersion: karpenter.sh/v1
      kind: NodePool
      metadata:
        name: default
      spec:
        # ë…¸ë“œí´ë˜ìŠ¤ ì°¸ì¡°
        template:
          metadata:
            labels:
              intent: apps
              nodepool: default
          spec:
            # ë…¸ë“œ ìš”êµ¬ì‚¬í•­
            requirements:
              - key: kubernetes.io/arch
                operator: In
                values: ["amd64"]
              - key: karpenter.sh/capacity-type
                operator: In
                values: ["on-demand"]
              - key: node.kubernetes.io/instance-type
                operator: In
                values: ["t3.small"]
            
            # EC2NodeClass ì°¸ì¡°
            nodeClassRef:
              group: karpenter.k8s.aws
              kind: EC2NodeClass
              name: default
        
        # ë¦¬ì†ŒìŠ¤ ì œí•œ
        limits:
          cpu: 20
          memory: 40Gi
        
        # ë…¸ë“œ ì¶•ì¶œ ì •ì±…
        disruption:
          # ë¹ˆ ë…¸ë“œ ì¶•ì¶œ ì‹œê°„
          consolidationPolicy: WhenEmptyOrUnderutilized
          consolidateAfter: 5m
      EOF

      # NodePool ìƒì„± í™•ì¸
      if kubectl get nodepool default; then
        echo "NodePool created successfully"
      else
        echo "Failed to create NodePool"
        exit 1
      fi

      echo "Waiting for resources to be created..."
      sleep 15

      echo "Karpenter v1.6.0 NodePool and EC2NodeClass created successfully"

      # ìƒì„±ëœ ë¦¬ì†ŒìŠ¤ í™•ì¸
      echo "Checking created resources..."
      kubectl get nodepools -o wide
      kubectl get ec2nodeclasses -o wide

      EOT
    ]
  }
}
```
## CRD (Custom Resource Definition)
### CRDë€?
- "ìƒˆë¡œìš´ ì–¸ì–´ ì‚¬ì „ ë§Œë“¤ê¸°" Kubernetesë¥¼ ì–¸ì–´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.

**ê¸°ë³¸ Kubernetes ë‹¨ì–´ë“¤ (ë‚´ì¥ ë¦¬ì†ŒìŠ¤):**
- Pod: "ì»¨í…Œì´ë„ˆ ì‹¤í–‰"
- Service: "ë„¤íŠ¸ì›Œí¬ ì—°ê²°"
- Deployment: "ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬"
- Node: "ì„œë²„"

**CRD = "ìƒˆë¡œìš´ ë‹¨ì–´ ì •ì˜ì„œ":**
- NodePool: "ë…¸ë“œ ê·¸ë£¹ ì •ì±…" (Karpenterê°€ ë§Œë“  ìƒˆ ë‹¨ì–´)
- EC2NodeClass: "AWS ë…¸ë“œ ì„¤ì •" (Karpenterê°€ ë§Œë“  ìƒˆ ë‹¨ì–´)

### Karpenterì—ì„œ CRDê°€ ì¤‘ìš”í•œ ì´ìœ  
#### 1. Karpenter Controller ì‘ë™ ë°©ì‹
```
Karpenter Controller ì‹œì‘
        â†“
"NodePool ë¦¬ì†ŒìŠ¤ ë³€í™” ê°ì§€í•˜ê² ì–´!"
        â†“
CRD ìˆë‚˜ í™•ì¸
        â†“
âŒ CRD ì—†ìŒ: "NodePoolì´ ë­”ì§€ ëª°ë¼ì„œ ê°ì§€í•  ìˆ˜ ì—†ì–´!" â†’ ì¤‘ë‹¨
âœ… CRD ìˆìŒ: "NodePool ë³€í™”ë¥¼ ê°ì§€í•˜ê³  ë°˜ì‘í•˜ê² ì–´!" â†’ ì •ìƒ ì‘ë™
```
#### 2. ì‹¤ì œ ë…¸ë“œ ìƒì„± í”„ë¡œì„¸ìŠ¤
```
ì‚¬ìš©ìê°€ NodePool ìƒì„±/ìˆ˜ì •
        â†“
Kubernetes API Server: "NodePool ë¦¬ì†ŒìŠ¤ê°€ ë³€ê²½ë˜ì—ˆë„¤"
        â†“
Karpenter Controller: "ë³€ê²½ ê°ì§€! ìƒˆë¡œìš´ ë…¸ë“œê°€ í•„ìš”í•œê°€?"
        â†“
EC2NodeClass ì°¸ì¡°: "ì–´ë–¤ ìŠ¤í™ì˜ ì„œë²„ë¥¼ ë§Œë“¤ì§€ í™•ì¸"
        â†“
AWS EC2 API í˜¸ì¶œ: "ì‹¤ì œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"
```
#### 3. CRDê°€ ì—†ë‹¤ë©´?
```
âŒ "NodePoolì´ ë­”ì§€ ëª°ë¼ìš”"
âŒ Controller ì•„ì˜ˆ ì‹œì‘ ì•ˆë¨
âŒ ë…¸ë“œ ìƒì„± ë¶ˆê°€ëŠ¥
```
### CRDì˜ ì‹¤í–‰ì„ ê¸°ë‹¤ë¦¼
```
echo "Waiting for Karpenter CRDs to be established..."
kubectl wait --for condition=established --timeout=300s crd/nodepools.karpenter.sh
kubectl wait --for condition=established --timeout=300s crd/ec2nodeclasses.karpenter.k8s.aws
```
**ì™œ ê¸°ë‹¤ë ¤ì•¼í• ê¹Œ?**
1. Karpenter Helm ì„¤ì¹˜ â†’ CRD ìƒì„± ì‹œì‘
2. CRD ì™„ì „ ë“±ë¡ê¹Œì§€ ì‹œê°„ ì†Œìš” (ë³´í†µ 30ì´ˆ~2ë¶„)
3. CRD ì¤€ë¹„ ì™„ë£Œ â†’ NodePool/EC2NodeClass ìƒì„± ê°€ëŠ¥
## EC2NodeClass
### EC2NodeClass = "ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ?" (HOW)
```
# EC2NodeClassëŠ” EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë°©ë²•ì„ ì •ì˜
spec:
  amiSelectorTerms: [...] # ì–´ë–¤ AMI ì‚¬ìš©í• ì§€
  userData: |             # ë¶€íŒ… ì‹œ ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸
    /etc/eks/bootstrap.sh
  blockDeviceMappings:    # ë””ìŠ¤í¬ êµ¬ì„±
    volumeSize: 20Gi
  metadataOptions:        # ë³´ì•ˆ ì„¤ì •
    httpTokens: required
```
### EC2NodeClassê°€ ë‹´ë‹¹í•˜ëŠ” ê²ƒë“¤
```
AWS EC2 ê´€ë ¨ ëª¨ë“  ì„¤ì •:
- AMI ì„ íƒ (ìš´ì˜ì²´ì œ)
- ë³´ì•ˆê·¸ë£¹ (ë„¤íŠ¸ì›Œí¬ ë°©í™”ë²½)
- ì„œë¸Œë„· (ë„¤íŠ¸ì›Œí¬ ìœ„ì¹˜)
- IAM ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ (ê¶Œí•œ)
- User Data (ë¶€íŒ… ìŠ¤í¬ë¦½íŠ¸)
- EBS ë³¼ë¥¨ (ìŠ¤í† ë¦¬ì§€)
- EC2 ë©”íƒ€ë°ì´í„° ì„¤ì •

â†’ "ë¬¼ë¦¬ì  ì„œë²„ë¥¼ ì–´ë–»ê²Œ êµ¬ì„±í•  ê²ƒì¸ê°€"
```
## NodePool
### NodePool = "ì–¸ì œ, ì–¼ë§ˆë‚˜ ë§Œë“¤ê¹Œ?" (WHEN & HOW MANY)
```
# NodePoolì€ ìŠ¤ì¼€ì¼ë§ ì¡°ê±´ê³¼ ì œì•½ì„ ì •ì˜
spec:
  requirements:           # ì–´ë–¤ ì¡°ê±´ì˜ ë…¸ë“œê°€ í•„ìš”í•œì§€
    - key: node.kubernetes.io/instance-type
      values: ["t3.small"]
  limits:                 # ìµœëŒ€ ì–¼ë§ˆë‚˜ ìƒì„±í• ì§€
    cpu: 20
  disruption:             # ì–¸ì œ ì‚­ì œí• ì§€
    consolidateAfter: 5m
```
### NodePoolì´ ë‹´ë‹¹í•˜ëŠ” ê²ƒë“¤
```
Kubernetes ìŠ¤ì¼€ì¼ë§ ê´€ë ¨ ëª¨ë“  ì •ì±…:
- ë…¸ë“œ ìš”êµ¬ì‚¬í•­ (ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…, ì•„í‚¤í…ì²˜ ë“±)
- ë¦¬ì†ŒìŠ¤ ì œí•œ (ìµœëŒ€ CPU/ë©”ëª¨ë¦¬)
- ìŠ¤ì¼€ì¼ë§ ì •ì±… (ì–¸ì œ ìƒì„±/ì‚­ì œ)
- ë…¸ë“œ ë¼ë²¨ë§
- Taints/Tolerations

â†’ "ì–¸ì œ ì–¼ë§ˆë‚˜ ë§Œë“¤ê³  ê´€ë¦¬í•  ê²ƒì¸ê°€"
```

## 1:N ê´€ê³„ ì´í•´
### í•˜ë‚˜ì˜ EC2NodeClass, ì—¬ëŸ¬ NodePool ê°€ëŠ¥
```
í•˜ë‚˜ì˜ EC2NodeClass:
- í‘œì¤€ AMI ì„¤ì •
- í‘œì¤€ ë³´ì•ˆ ì„¤ì •  
- í‘œì¤€ ë„¤íŠ¸ì›Œí¬ ì„¤ì •

ì—¬ëŸ¬ NodePool:
- ê°œë°œìš© (ì‘ì€ ì¸ìŠ¤í„´ìŠ¤)
- ìš´ì˜ìš© (í° ì¸ìŠ¤í„´ìŠ¤)  
- GPUìš© (GPU ì¸ìŠ¤í„´ìŠ¤)
- Spotìš© (ë¹„ìš© ì ˆì•½)

â†’ ê³µí†µ ì¸í”„ë¼ ì„¤ì •ì€ ì¬ì‚¬ìš©í•˜ë©´ì„œ 
  ìš©ë„ë³„ ìŠ¤ì¼€ì¼ë§ ì •ì±…ì€ ë¶„ë¦¬
```
## EC2NodeClass VS NodePool
| êµ¬ë¶„ | EC2NodeClass | NodePool |
| :--- | :--- | :--- |
| **ì—­í• ** | AWS ì¸í”„ë¼ í…œí”Œë¦¿ | Kubernetes ìŠ¤ì¼€ì¼ë§ ì •ì±… |
| **ê´€ì‹¬ì‚¬** | HOW (ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ) | WHEN/HOW MANY (ì–¸ì œ/ì–¼ë§ˆë‚˜) |
| **ì„¤ì • ë‚´ìš©** | AMI, ë³´ì•ˆê·¸ë£¹, ì„œë¸Œë„·, ìŠ¤í† ë¦¬ì§€ | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…, ë¦¬ì†ŒìŠ¤ ì œí•œ, ìŠ¤ì¼€ì¼ë§ |
| **ë³€ê²½ ë¹ˆë„** | ë‚®ìŒ (ì¸í”„ë¼ í‘œì¤€) | ë†’ìŒ (ì›Œí¬ë¡œë“œë³„ ìš”êµ¬ì‚¬í•­) |
| **ì¬ì‚¬ìš©ì„±** | ë†’ìŒ (ì—¬ëŸ¬ NodePoolì´ ì°¸ì¡°) | ë‚®ìŒ (íŠ¹ì • ìš©ë„) |