# EKS HPA + Karpenter autoscailing

ê²Œì‹œì¼: Tue, 26 Aug 2025 06:24:11 GMT
ë§í¬: https://velog.io/@agline/EKS-HPA-Karpenter-autoscailing

---

<p>í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìš´ì˜í•˜ë‹¤ ë³´ë©´, <strong>'ë¦¬ì†ŒìŠ¤ ê´€ë¦¬'</strong>
ë¼ëŠ” í’€ë¦¬ì§€ ì•ŠëŠ” ìˆ™ì œì™€ ë§ˆì£¼í•˜ê²Œ ë©ë‹ˆë‹¤. 
ê°‘ì‘ìŠ¤ëŸ¬ìš´ íŠ¸ë˜í”½ í­ì¦ìœ¼ë¡œ ì„œë¹„ìŠ¤ê°€ ë§ˆë¹„ë˜ê±°ë‚˜, ë°˜ëŒ€ë¡œ í•œì‚°í•œ ì‹œê°„ì—ë„ ë¹„ì‹¼ ìì›ì„ ë‚­ë¹„í•˜ê³  ìˆì§€ëŠ” ì•Šìœ¼ì‹ ê°€ìš”? 
ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ì— ì‹¤íŒ¨í•˜ì—¬ ìƒˆë²½ì— ê¸´ê¸‰ ëŒ€ì‘ì„ í•˜ê±°ë‚˜, ë§¤ë‹¬ ë‚ ì•„ì˜¤ëŠ” ì²­êµ¬ì„œì— í•œìˆ¨ ì‰¬ëŠ” ê²½í—˜ì€ ì´ì œ ê·¸ë§Œí•  ë•Œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<p>ì´ëŸ¬í•œ ê³ ë¯¼ì„ í•´ê²°í•  ì—´ì‡ ëŠ” ë°”ë¡œ <strong>'ì˜¤í† ìŠ¤ì¼€ì¼ë§'</strong> ì— ìˆìŠµë‹ˆë‹¤. 
ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ <strong>HPA(Horizontal Pod Autoscaler)</strong> ì™€ <strong>Karpenter</strong> ë¥¼ ì¡°í•©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë¶€í•˜ì— ë”°ë¼ íŒŒë“œ(Pod)ì™€ ë…¸ë“œ(Node)ê°€ ì•Œì•„ì„œ ëŠ˜ì–´ë‚˜ê³  ì¤„ì–´ë“œëŠ” <strong>'ì™„ì „ ìë™í™”ëœ ì˜¤í† ìŠ¤ì¼€ì¼ë§'</strong> ì‹œìŠ¤í…œì„ Terraformìœ¼ë¡œ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ê³µìœ í•©ë‹ˆë‹¤.</p>
<ul>
<li><strong>HPA (Horizontal Pod Autoscaler)</strong> : ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ CPU, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê°ì§€í•˜ì—¬ íŒŒë“œ(Pod)ì˜ ê°œìˆ˜ë¥¼ ì‹ ì†í•˜ê²Œ ì¡°ì ˆí•©ë‹ˆë‹¤.</li>
<li><strong>Karpenter</strong> : HPAë¡œ ì¸í•´ ëŠ˜ì–´ë‚œ íŒŒë“œë¥¼ ë°°ì¹˜í•  ê³µê°„ì´ ë¶€ì¡±í•  ë•Œ, í•„ìš”í•œ ì‚¬ì–‘ì˜ ë…¸ë“œ(EC2 ì¸ìŠ¤í„´ìŠ¤)ë¥¼ ì¦‰ì‹œ ìƒì„±í•˜ê³ , ë°˜ëŒ€ë¡œ ë¶ˆí•„ìš”í•´ì§„ ë…¸ë“œëŠ” ì œê±°í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.</li>
</ul>
<h1 id="ì „ì²´ì•„í‚¤í…ì³">ì „ì²´ì•„í‚¤í…ì³</h1>
<ol>
<li><p>Terraformì„ ì‚¬ìš©í•˜ì—¬ AWSì˜ ê¸°ë³¸ ì¸í”„ë¼(VPC, Subnet, NAT Gateway, Bastion Host, RDS)ì™€ EKS í´ëŸ¬ìŠ¤í„°ë¥¼ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.</p>
</li>
<li><p>Bastion Hostë¥¼ í†µí•´ EKS í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•˜ì—¬ AWS Load Balancer Controller, Metrics Server, Karpenterë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.</p>
</li>
<li><p>ì• í”Œë¦¬ì¼€ì´ì…˜(Frontend, Backend)ì„ ë°°í¬í•˜ê³ , ì™¸ë¶€ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ Ingressë¥¼ ìƒì„±í•©ë‹ˆë‹¤.</p>
</li>
<li><p>Backend Deploymentì— HPAë¥¼ ì ìš©í•˜ì—¬ CPU/Memory ì‚¬ìš©ëŸ‰ì— ë”°ë¼ íŒŒë“œê°€ ìë™ìœ¼ë¡œ í™•ì¥/ì¶•ì†Œë˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.</p>
</li>
<li><p>KarpenterëŠ” ìŠ¤ì¼€ì¤„ë§ ëŒ€ê¸° ì¤‘ì¸ íŒŒë“œë¥¼ ê°ì§€í•˜ê³ , NodePool ì„¤ì •ì— ë”°ë¼ ìµœì ì˜ EC2 ì¸ìŠ¤í„´ìŠ¤ë¥¼ í”„ë¡œë¹„ì €ë‹í•˜ì—¬ íŒŒë“œë¥¼ ë°°ì¹˜í•©ë‹ˆë‹¤.</p>
</li>
</ol>
<p><img alt="" src="https://velog.velcdn.com/images/agline/post/20b0dff3-e0cb-4334-9eb0-021346343a88/image.png" /></p>
<h1 id="1ë‹¨ê³„-terraformìœ¼ë¡œ-ì¸í”„ë¼-êµ¬ì¶•í•˜ê¸°">1ë‹¨ê³„: Terraformìœ¼ë¡œ ì¸í”„ë¼ êµ¬ì¶•í•˜ê¸°</h1>
<h2 id="vpc-subnet-ê°€ìš©ì˜ì—­-ab-ë‘ê°œ">VPC, subnet ê°€ìš©ì˜ì—­ a,b ë‘ê°œ</h2>
<pre><code>provider &quot;aws&quot; {
  region = &quot;ap-northeast-2&quot;
}

data &quot;aws_availability_zones&quot; &quot;available&quot; {}

locals {
  azs = slice(data.aws_availability_zones.available.names, 0, 2)
}

# VPC
resource &quot;aws_vpc&quot; &quot;main&quot; {
  cidr_block           = &quot;10.0.0.0/16&quot;
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = &quot;my-vpc&quot;
  }
}

# ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´
resource &quot;aws_internet_gateway&quot; &quot;igw&quot; {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = &quot;my-igw&quot;
  }
}

# í¼ë¸”ë¦­ ì„œë¸Œë„·
resource &quot;aws_subnet&quot; &quot;public&quot; {
  count                   = 2
  vpc_id                  = aws_vpc.main.id
  cidr_block              = element([&quot;10.0.1.0/24&quot;, &quot;10.0.2.0/24&quot;], count.index)
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name = element([&quot;my-pub-2a&quot;, &quot;my-pub-2b&quot;], count.index)
  }
}

# í”„ë¼ì´ë¹— ì„œë¸Œë„·
resource &quot;aws_subnet&quot; &quot;private&quot; {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = element([&quot;10.0.101.0/24&quot;, &quot;10.0.102.0/24&quot;], count.index)
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = element([&quot;my-pvt-2a&quot;, &quot;my-pvt-2b&quot;], count.index)
  }
}

# í¼ë¸”ë¦­ NACL
resource &quot;aws_network_acl&quot; &quot;public&quot; {
  vpc_id = aws_vpc.main.id

  egress {
    protocol   = &quot;-1&quot;
    rule_no    = 100
    action     = &quot;allow&quot;
    cidr_block = &quot;0.0.0.0/0&quot;
    from_port  = 0
    to_port    = 0
  }

  ingress {
    protocol   = &quot;-1&quot;
    rule_no    = 100
    action     = &quot;allow&quot;
    cidr_block = &quot;0.0.0.0/0&quot;
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = &quot;my-nacl-pub&quot;
  }
}

# í”„ë¼ì´ë¹— NACL
resource &quot;aws_network_acl&quot; &quot;private&quot; {
  vpc_id = aws_vpc.main.id

  egress {
    protocol   = &quot;-1&quot;
    rule_no    = 100
    action     = &quot;allow&quot;
    cidr_block = &quot;0.0.0.0/0&quot;
    from_port  = 0
    to_port    = 0
  }

  ingress {
    protocol   = &quot;-1&quot;
    rule_no    = 100
    action     = &quot;allow&quot;
    cidr_block = &quot;0.0.0.0/0&quot;
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = &quot;my-nacl-pvt&quot;
  }
}

# í¼ë¸”ë¦­ ì„œë¸Œë„·ì— í¼ë¸”ë¦­ NACL ì—°ê²°
resource &quot;aws_network_acl_association&quot; &quot;public&quot; {
  count          = 2
  subnet_id      = aws_subnet.public[count.index].id
  network_acl_id = aws_network_acl.public.id
}

# í”„ë¼ì´ë¹— ì„œë¸Œë„·ì— í”„ë¼ì´ë¹— NACL ì—°ê²°
resource &quot;aws_network_acl_association&quot; &quot;private&quot; {
  count          = 2
  subnet_id      = aws_subnet.private[count.index].id
  network_acl_id = aws_network_acl.private.id
}

# í¼ë¸”ë¦­ ë¼ìš°íŒ… í…Œì´ë¸”
resource &quot;aws_route_table&quot; &quot;public&quot; {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = &quot;0.0.0.0/0&quot;
    gateway_id = aws_internet_gateway.igw.id
  }

  tags = {
    Name = &quot;my-rtb-pub&quot;
  }
}

# í”„ë¼ì´ë¹— ë¼ìš°íŒ… í…Œì´ë¸” (ê¸°ë³¸ ì„¤ì •)
resource &quot;aws_route_table&quot; &quot;private&quot; {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = &quot;my-rtb-pvt&quot;
  }
}

# í¼ë¸”ë¦­ ì„œë¸Œë„·ì— ë¼ìš°íŒ… í…Œì´ë¸” ì—°ê²°
resource &quot;aws_route_table_association&quot; &quot;public&quot; {
  count          = 2
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

# í”„ë¼ì´ë¹— ì„œë¸Œë„·ì— ë¼ìš°íŒ… í…Œì´ë¸” ì—°ê²°
resource &quot;aws_route_table_association&quot; &quot;private&quot; {
  count          = 2
  subnet_id      = aws_subnet.private[count.index].id
  route_table_id = aws_route_table.private.id
}

#ì¶œë ¥
output &quot;public_subnet_ids&quot; {
  value = aws_subnet.public[*].id
}

output &quot;private_subnet_ids&quot; {
  value = aws_subnet.private[*].id
}</code></pre><h2 id="eks-ì„¤ì •ì„-ìœ„í•œ-eip-nat-gateway">EKS ì„¤ì •ì„ ìœ„í•œ EIP, NAT Gateway</h2>
<pre><code>locals {
  vpc_id           = &quot;vpc-example&quot;
  public_subnet_ids = [&quot;subnet-example&quot;, &quot;subnet-example&quot;]
  private_subnet_ids = [&quot;subnet-example&quot;, &quot;subnet-example&quot;]
}

# 1. Elastic IP for NAT
resource &quot;aws_eip&quot; &quot;nat_eip&quot; {}

# 2. NAT Gateway (í¼ë¸”ë¦­ ì„œë¸Œë„·ì— ìƒì„±í•´ì•¼ í•¨)
resource &quot;aws_nat_gateway&quot; &quot;nat&quot; {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = &quot;subnet-example&quot;  # í¼ë¸”ë¦­ ì„œë¸Œë„· ì¤‘ í•˜ë‚˜
  tags = {
    Name = &quot;my-nat-gw&quot;
  }
}

# 3. í”„ë¼ì´ë¹— ë¼ìš°íŒ… í…Œì´ë¸” NAT Gateway ë¼ìš°íŒ…
resource &quot;aws_route&quot; &quot;private&quot; {
  route_table_id       = &quot;rtb-example&quot;
  destination_cidr_block = &quot;0.0.0.0/0&quot;
  nat_gateway_id       = aws_nat_gateway.nat.id
}</code></pre><h2 id="eksì ‘ê·¼ì„-ìœ„í•œ-bastion-ec2">EKSì ‘ê·¼ì„ ìœ„í•œ Bastion EC2</h2>
<pre><code>provider &quot;aws&quot; {
  region = &quot;ap-northeast-2&quot;

  access_key = var.aws_access_key_id
  secret_key = var.aws_secret_access_key
}

##########################
# Bastion ë³´ì•ˆ ê·¸ë£¹
##########################

resource &quot;aws_security_group&quot; &quot;bastion_sg&quot; {
  name        = &quot;bastion-sg&quot;
  description = &quot;Allow SSH&quot;
  vpc_id      = local.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags = {
    Name = &quot;bastion-sg&quot;
  }
}

resource &quot;aws_iam_role&quot; &quot;bastion_role&quot; {
  name = &quot;bastionRole&quot;

  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [{
      Effect = &quot;Allow&quot;,
      Principal = {
        Service = &quot;ec2.amazonaws.com&quot;
      },
      Action = &quot;sts:AssumeRole&quot;
    }]
  })
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;bastion_policy&quot; {
  for_each = toset([
    &quot;arn:aws:iam::aws:policy/AmazonEKSClusterPolicy&quot;,
    &quot;arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy&quot;,
    &quot;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly&quot;,
    &quot;arn:aws:iam::aws:policy/AdministratorAccess&quot;
  ])
  role       = aws_iam_role.bastion_role.name
  policy_arn = each.value
}

resource &quot;aws_iam_instance_profile&quot; &quot;bastion_profile&quot; {
  name = &quot;bastionInstanceProfile&quot;
  role = aws_iam_role.bastion_role.name
}

##########################
# Bastion EC2 ì¸ìŠ¤í„´ìŠ¤
##########################

resource &quot;aws_instance&quot; &quot;bastion&quot; {
  ami                   = &quot;ami-0fc8aeaa301af7663&quot;
  instance_type         = &quot;t3.micro&quot;
  subnet_id             = local.public_subnet_ids[0]
  key_name              = &quot;bastion-key&quot;
  vpc_security_group_ids = [aws_security_group.bastion_sg.id]
  associate_public_ip_address = true
  iam_instance_profile        = aws_iam_instance_profile.bastion_profile.name

  user_data = &lt;&lt;-EOF
              #!/bin/bash
              yum update -y
              yum install -y unzip curl wget
              # AWS CLI v2 ì„¤ì¹˜
              curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot;
              unzip awscliv2.zip
              ./aws/install
              # MariaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜
              yum install -y mariadb105
              # git ì„¤ì¹˜
              sudo yum install -y git
              # kubectl ì„¤ì¹˜
              curl -LO https://dl.k8s.io/release/v1.33.0/bin/linux/amd64/kubectl
              chmod +x kubectl
              mv kubectl /usr/local/bin/
              # k9s ì„¤ì¹˜
              K9S_URL=$(curl -s https://api.github.com/repos/derailed/k9s/releases/latest \
                | grep &quot;browser_download_url.*Linux_amd64.tar.gz&quot; \
                | cut -d '&quot;' -f 4 \
                | head -n 1)
              wget &quot;$K9S_URL&quot; -O k9s_Linux_amd64.tar.gz
              tar -xvf k9s_Linux_amd64.tar.gz
              mv k9s /usr/local/bin/
              rm -f k9s_Linux_amd64.tar.gz
              # AWS CLI ê¸°ë³¸ ì„¤ì •
              mkdir -p /home/ec2-user/.aws
              cat &gt; /home/ec2-user/.aws/credentials &lt;&lt;EOL
              [default]
              aws_access_key_id = ${var.aws_access_key_id}
              aws_secret_access_key = ${var.aws_secret_access_key}
              EOL
              cat &gt; /home/ec2-user/.aws/config &lt;&lt;EOL
              [default]
              region = ap-northeast-2
              output = json
              EOL
              chown -R ec2-user:ec2-user /home/ec2-user/.aws
              EOF

  tags = {
    Name = &quot;bastion&quot;
  }
}</code></pre><h2 id="rds">RDS</h2>
<pre><code>##########################
# RDS ë³´ì•ˆ ê·¸ë£¹ (MariaDBìš©)
##########################

resource &quot;aws_security_group&quot; &quot;rds_sg&quot; {
  name   = &quot;rds-sg&quot;
  vpc_id = local.vpc_id

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = &quot;tcp&quot;
    security_groups = [aws_security_group.bastion_sg.id]  
    description     = Bastionì—ì„œ ì ‘ê·¼ í—ˆìš©
  }

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = &quot;tcp&quot;
    security_groups = [aws_eks_cluster.main.vpc_config[0].cluster_security_group_id]
    description     = EKS í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ í—ˆìš©
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags = {
    Name = &quot;rds-sg&quot;
  }
}

##########################
# RDS ì„œë¸Œë„· ê·¸ë£¹
##########################

resource &quot;aws_db_subnet_group&quot; &quot;mariadb_subnet_group&quot; {
  name       = &quot;mariadb-subnet-group&quot;
  subnet_ids = local.private_subnet_ids

  tags = {
    Name = &quot;mariadb-subnet-group&quot;
  }
}

##########################
# RDS ì¸ìŠ¤í„´ìŠ¤ (MariaDB)
##########################

resource &quot;aws_db_instance&quot; &quot;mariadb&quot; {
  identifier              = &quot;mariadb-instance&quot;
  allocated_storage       = 20
  engine                  = &quot;mariadb&quot;
  engine_version          = &quot;10.6&quot;
  instance_class          = &quot;db.t3.small&quot;
  db_name                 = &quot;mydb&quot;
  username                = &quot;admin&quot;
  password                = &quot;MySecurePass123!&quot;  # í…ŒìŠ¤íŠ¸ìš©. ìš´ì˜ ì‹œ Secrets Manager ì‚¬ìš© ê¶Œì¥
  skip_final_snapshot     = true
  publicly_accessible     = false
  vpc_security_group_ids  = [aws_security_group.rds_sg.id]
  db_subnet_group_name    = aws_db_subnet_group.mariadb_subnet_group.name

  tags = {
    Name = &quot;mariadb&quot;
  }
}</code></pre><h2 id="eks-cluster-nodegroup-ìƒì„±">EKS cluster, NodeGroup ìƒì„±</h2>
<pre><code>##########################
# EKS í´ëŸ¬ìŠ¤í„° IAM Role
##########################

resource &quot;aws_iam_role&quot; &quot;eks_role&quot; {
  name = &quot;eksClusterRole&quot;

  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [{
      Action = &quot;sts:AssumeRole&quot;,
      Effect = &quot;Allow&quot;,
      Principal = {
        Service = &quot;eks.amazonaws.com&quot;
      }
    }]
  })
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;eks_attach&quot; {
  role       = aws_iam_role.eks_role.name
  policy_arn = &quot;arn:aws:iam::aws:policy/AmazonEKSClusterPolicy&quot;
}

##########################
# EKS í´ëŸ¬ìŠ¤í„°
##########################

resource &quot;aws_eks_cluster&quot; &quot;main&quot; {
  name     = &quot;my-eks-cluster&quot;
  role_arn = aws_iam_role.eks_role.arn

  vpc_config {
    subnet_ids = local.private_subnet_ids
  }

  depends_on = [aws_iam_role_policy_attachment.eks_attach]
}

##########################
# EKS ë…¸ë“œ ê·¸ë£¹ IAM Role &amp; Instance Profile
##########################

resource &quot;aws_iam_role&quot; &quot;eks_node_role&quot; {
  name = &quot;eksNodeGroupRole&quot;

  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [{
      Effect = &quot;Allow&quot;,
      Principal = {
        Service = &quot;ec2.amazonaws.com&quot;
      },
      Action = &quot;sts:AssumeRole&quot;
    }]
  })
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;eks_node_policy&quot; {
  for_each = toset([
    &quot;arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy&quot;,
    &quot;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly&quot;,
    &quot;arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy&quot;
  ])

  role       = aws_iam_role.eks_node_role.name
  policy_arn = each.value
}

# Karpenterì—ì„œ ì°¸ì¡°í•  EKS ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ì„ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±
resource &quot;aws_iam_instance_profile&quot; &quot;eks_node_profile&quot; {
  name = &quot;eksNodeGroupProfile&quot;
  role = aws_iam_role.eks_node_role.name
}

##########################
# EKS Managed Node Group
##########################

resource &quot;aws_eks_node_group&quot; &quot;node_group&quot; {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = &quot;my-node-group&quot;
  node_role_arn   = aws_iam_role.eks_node_role.arn
  subnet_ids      = local.private_subnet_ids

  scaling_config {
    desired_size = 1
    max_size     = 3
    min_size     = 1
  }

  instance_types = [&quot;t3.small&quot;]

  disk_size = 20

  depends_on = [
    aws_iam_role_policy_attachment.eks_node_policy,
    aws_eks_cluster.main
  ]

  tags = {
    Name = &quot;eks-node-group&quot;
  }
}</code></pre><h1 id="2ë‹¨ê³„-í´ëŸ¬ìŠ¤í„°ì—-ì£¼ìš”-ì»´í¬ë„ŒíŠ¸-ì„¤ì¹˜í•˜ê¸°">2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ì— ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ì¹˜í•˜ê¸°</h1>
<h2 id="aws-load-balancer-controller-lbc">AWS Load Balancer Controller (LBC)</h2>
<p><a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/main/docs/install/iam_policy.json">https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/main/docs/install/iam_policy.json</a></p>
<pre><code>##########################
# LBC IAM
##########################
data &quot;aws_eks_cluster&quot; &quot;cluster&quot; {
  name = aws_eks_cluster.main.name
}

data &quot;aws_eks_cluster_auth&quot; &quot;cluster&quot; {
  name = aws_eks_cluster.main.name
}

data &quot;tls_certificate&quot; &quot;eks_cluster&quot; {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
}

resource &quot;aws_iam_openid_connect_provider&quot; &quot;oidc_provider&quot; {
  url           = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer
  client_id_list  = [&quot;sts.amazonaws.com&quot;]
  thumbprint_list = [data.tls_certificate.eks_cluster.certificates[0].sha1_fingerprint]
}

resource &quot;aws_iam_policy&quot; &quot;lbc_policy&quot; {
  name        = &quot;AWSLoadBalancerControllerIAMPolicy&quot;
  description = &quot;Policy for AWS Load Balancer Controller&quot;
  policy      = file(&quot;iam_policy_lbc.json&quot;)
}

resource &quot;aws_iam_role&quot; &quot;lbc_sa_role&quot; {
  name = &quot;lbc-sa-role&quot;

  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [{
      Effect    = &quot;Allow&quot;,
      Principal = { Federated = aws_iam_openid_connect_provider.oidc_provider.arn },
      Action    = &quot;sts:AssumeRoleWithWebIdentity&quot;,
      Condition = {
        StringEquals = {
          &quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, &quot;https://&quot;, &quot;&quot;)}:sub&quot; = &quot;system:serviceaccount:kube-system:aws-load-balancer-controller&quot;
        }
      }
    }]
  })
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;lbc_attach&quot; {
  role       = aws_iam_role.lbc_sa_role.name
  policy_arn = aws_iam_policy.lbc_policy.arn
}

##########################
# LBC ì„¤ì¹˜
##########################
resource &quot;null_resource&quot; &quot;install_lbc&quot; {
  depends_on = [
    aws_instance.bastion,
    aws_eks_cluster.main,
    aws_eks_node_group.node_group,
    aws_iam_role_policy_attachment.lbc_attach
  ]

  connection {
    type        = &quot;ssh&quot;
    host        = aws_instance.bastion.public_ip
    user        = &quot;ec2-user&quot;
    private_key = file(&quot;C:/.ssh/bastion-key.pem&quot;)
    timeout     = &quot;10m&quot;
  }

  provisioner &quot;remote-exec&quot; {
    inline = [
      &quot;until aws eks --region ap-northeast-2 describe-cluster --name my-eks-cluster --query cluster.status --output text | grep -q 'ACTIVE'; do echo 'Waiting for EKS cluster to become active...'; sleep 30; done&quot;,
      &quot;aws eks update-kubeconfig --region ap-northeast-2 --name my-eks-cluster&quot;,
      &quot;curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash&quot;,
      &quot;helm repo add eks https://aws.github.io/eks-charts&quot;,
      &quot;helm repo update&quot;,

      &quot;kubectl create ns kube-system || true&quot;,

      # ê¸°ì¡´ ì„¤ì¹˜ ì™„ì „íˆ ì •ë¦¬
      &quot;echo 'Cleaning up existing installation...'&quot;,
      &quot;helm uninstall aws-load-balancer-controller -n kube-system || true&quot;,
      &quot;kubectl delete deployment aws-load-balancer-controller -n kube-system || true&quot;,
      &quot;kubectl delete serviceaccount aws-load-balancer-controller -n kube-system || true&quot;,
      &quot;kubectl delete secrets -l name=aws-load-balancer-controller -n kube-system || true&quot;,

      &lt;&lt;-EOT
      cat &lt;&lt;EOF | kubectl apply -f -
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: aws-load-balancer-controller
        namespace: kube-system
        annotations:
          eks.amazonaws.com/role-arn: ${aws_iam_role.lbc_sa_role.arn}
        labels:
          app.kubernetes.io/name: aws-load-balancer-controller
          app.kubernetes.io/component: controller
      EOF
      EOT
      ,

      # Helmìœ¼ë¡œ ì„¤ì¹˜ (ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸ëŠ” ìƒì„±í•˜ì§€ ì•Šê³  ê¸°ì¡´ ê²ƒ ì‚¬ìš©)
      &quot;helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=my-eks-cluster --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller --set region=ap-northeast-2 --set vpcId=${local.vpc_id}&quot;
    ]
  }
}</code></pre><h2 id="metrics-server">Metrics Server</h2>
<pre><code>############################################
# Metrics Server Helm ì„¤ì¹˜
############################################
resource &quot;null_resource&quot; &quot;install_metrics_server&quot; {
  depends_on = [
    null_resource.install_karpenter
  ]

  connection {
    type        = &quot;ssh&quot;
    host        = aws_instance.bastion.public_ip
    user        = &quot;ec2-user&quot;
    private_key = file(&quot;C:/.ssh/bastion-key.pem&quot;)
  }

  provisioner &quot;remote-exec&quot; {
    inline = [
      &quot;helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/&quot;,
      &quot;helm repo update&quot;,
      &quot;helm upgrade --install metrics-server metrics-server/metrics-server --namespace kube-system&quot;,
      &quot;echo 'Metrics Server installation complete.'&quot;
    ]
  }
}</code></pre><h2 id="karpenter">Karpenter</h2>
<p><a href="https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/#create-the-karpentercontroller-iam-role">https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/#create-the-karpentercontroller-iam-role</a></p>
<pre><code>############################################
# Karpenter IAM Role &amp; Policy
############################################
resource &quot;aws_iam_role&quot; &quot;karpenter_controller_role&quot; {
  name = &quot;KarpenterControllerRole&quot;

  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [{
      Effect = &quot;Allow&quot;,
      Principal = {
        Federated = aws_iam_openid_connect_provider.oidc_provider.arn
      },
      Action = &quot;sts:AssumeRoleWithWebIdentity&quot;,
      Condition = {
        StringEquals = {
          &quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, &quot;https://&quot;, &quot;&quot;)}:sub&quot; = &quot;system:serviceaccount:karpenter:karpenter&quot;
        }
      }
    }]
  })
}

data &quot;aws_caller_identity&quot; &quot;current&quot; {}

resource &quot;aws_iam_policy&quot; &quot;karpenter_controller_policy&quot; {
  name   = &quot;KarpenterControllerPolicy&quot;
  policy = templatefile(&quot;karpenter-controller-policy.json&quot;, {
    AWS_PARTITION  = &quot;aws&quot;
    AWS_ACCOUNT_ID = data.aws_caller_identity.current.account_id
    AWS_REGION     = &quot;ap-northeast-2&quot;
    CLUSTER_NAME   = aws_eks_cluster.main.name
  })
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;karpenter_controller_attach&quot; {
  role       = aws_iam_role.karpenter_controller_role.name
  policy_arn = aws_iam_policy.karpenter_controller_policy.arn
}

############################################
# ì„œë¸Œë„·ê³¼ ë³´ì•ˆê·¸ë£¹ì— Karpenter íƒœê·¸ ì¶”ê°€
############################################
resource &quot;aws_ec2_tag&quot; &quot;private_subnet_tags&quot; {
  count       = length(local.private_subnet_ids)
  resource_id = local.private_subnet_ids[count.index]
  key         = &quot;karpenter.sh/discovery&quot;
  value       = aws_eks_cluster.main.name
}

# EKS í´ëŸ¬ìŠ¤í„°ì˜ ë³´ì•ˆê·¸ë£¹ì— íƒœê·¸ ì¶”ê°€
resource &quot;aws_ec2_tag&quot; &quot;cluster_security_group_tag&quot; {
  resource_id = aws_eks_cluster.main.vpc_config[0].cluster_security_group_id
  key         = &quot;karpenter.sh/discovery&quot;
  value       = aws_eks_cluster.main.name
}

############################################
# Karpenter Helm ì„¤ì¹˜
############################################
resource &quot;null_resource&quot; &quot;install_karpenter&quot; {
  depends_on = [
    aws_instance.bastion,
    aws_eks_cluster.main,
    aws_eks_node_group.node_group,
    null_resource.install_lbc,
    aws_ec2_tag.private_subnet_tags,
    aws_ec2_tag.cluster_security_group_tag
  ]

  connection {
    type        = &quot;ssh&quot;
    host        = aws_instance.bastion.public_ip
    user        = &quot;ec2-user&quot;
    private_key = file(&quot;C:/.ssh/bastion-key.pem&quot;)
  }

  provisioner &quot;remote-exec&quot; {
    inline = [
      &lt;&lt;EOT
      # LBC deploymentê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
      kubectl wait --namespace=kube-system deployment/aws-load-balancer-controller --for=condition=Available=True --timeout=5m

      helm repo add karpenter https://charts.karpenter.sh/
      helm repo update
      kubectl create namespace karpenter || true

      # Karpenter v1.6.0 ì„¤ì¹˜
      helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.6.0 \
        --namespace karpenter \
        --create-namespace \
        --set &quot;serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn=${aws_iam_role.karpenter_controller_role.arn}&quot; \
        --set &quot;settings.clusterName=${aws_eks_cluster.main.name}&quot; \
        --set &quot;settings.clusterEndpoint=${aws_eks_cluster.main.endpoint}&quot; \
        --set &quot;settings.defaultInstanceProfile=${aws_iam_instance_profile.eks_node_profile.name}&quot; \
        --set &quot;tolerations[0].key=karpenter.sh/unschedulable&quot; \
        --set &quot;tolerations[0].operator=Exists&quot; \
        --set &quot;tolerations[0].effect=NoSchedule&quot; \
        --set &quot;replicas=1&quot; \
        --set &quot;topologySpreadConstraints[0].maxSkew=2&quot; \
        --set &quot;topologySpreadConstraints[0].topologyKey=topology.kubernetes.io/zone&quot; \
        --set &quot;topologySpreadConstraints[0].whenUnsatisfiable=ScheduleAnyway&quot; \
        --set &quot;topologySpreadConstraints[0].labelSelector.matchLabels.app\\.kubernetes\\.io/name=karpenter&quot;
      EOT
    ]
  }
}</code></pre><h3 id="1-karpenter-iam-role--policy">1. Karpenter IAM Role &amp; Policy</h3>
<pre><code>resource &quot;aws_iam_role&quot; &quot;karpenter_controller_role&quot; {
  name = &quot;KarpenterControllerRole&quot;

  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [{
      Effect = &quot;Allow&quot;,
      Principal = {
        Federated = aws_iam_openid_connect_provider.oidc_provider.arn
      },
      Action = &quot;sts:AssumeRoleWithWebIdentity&quot;,
      Condition = {
        StringEquals = {
          &quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, &quot;https://&quot;, &quot;&quot;)}:sub&quot; = &quot;system:serviceaccount:karpenter:karpenter&quot;
        }
      }
    }]
  })
}</code></pre><ul>
<li><p>Principal: Federated = aws_iam_openid_connect_provider.oidc_provider.arn</p>
<ul>
<li>EKS í´ëŸ¬ìŠ¤í„°ì˜ OIDC Identity Providerë¥¼ í†µí•´ ì¸ì¦</li>
</ul>
</li>
<li><p>Action: sts:AssumeRoleWithWebIdentity</p>
<ul>
<li>Kubernetes ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸ê°€ AWS IAM ì—­í• ì„ assumeí•  ìˆ˜ ìˆê²Œ í•¨</li>
</ul>
</li>
<li><p>Condition: StringEquals</p>
<ul>
<li>íŠ¹ì • ì„œë¹„ìŠ¤ ì–´ì¹´ìš´íŠ¸(system:serviceaccount:karpenter:karpenter)ë§Œ ì´ ì—­í• ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì œí•œ</li>
</ul>
</li>
</ul>
<h4 id="eksì—ì„œ-oidcë¥¼-ì™œ-ì‚¬ìš©í• ê¹Œ">EKSì—ì„œ OIDCë¥¼ ì™œ ì‚¬ìš©í• ê¹Œ?</h4>
<p><strong>OIDC (OpenID Connect)</strong>ëŠ” OAuth 2.0 ìœ„ì— êµ¬ì¶•ëœ ì‹ ì› ì¸ì¦ í”„ë¡œí† ì½œì…ë‹ˆë‹¤.</p>
<blockquote>
<p>ğŸ¤” Kubernetes Podì´ AWS ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´?
   â†’ AWS ì•¡ì„¸ìŠ¤ í‚¤ê°€ í•„ìš”
   â†’ í•˜ì§€ë§Œ Podì— í‚¤ë¥¼ í•˜ë“œì½”ë”©í•˜ëŠ” ê²ƒì€ ë³´ì•ˆìƒ ìœ„í—˜!
   âœ… Pod â†’ Kubernetes Service Account â†’ AWS IAM Role
   ì•ˆì „í•˜ê³  ìë™í™”ëœ ë°©ì‹ìœ¼ë¡œ AWS ê¶Œí•œ íšë“!</p>
</blockquote>
<pre><code>data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer

ì‹¤ì œ ê°’ ì˜ˆì‹œ
https://oidc.eks.ap-northeast-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E

URLì—ì„œ &quot;https://&quot; ì œê±°
replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, &quot;https://&quot;, &quot;&quot;):sub

ê²°ê³¼
oidc.eks.ap-northeast-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E:sub

ì‹¤ì œ ì™„ì„±ëœ ì¡°ê±´ë¬¸
{
  &quot;oidc.eks.ap-northeast-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E:sub&quot;: &quot;system:serviceaccount:karpenter:karpenter&quot;
}</code></pre><blockquote>
<p>ğŸ”’ ì´ ì¡°ê±´ë¬¸ì˜ ë³´ì•ˆ íš¨ê³¼:
âŒ ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ Service Account â†’ ì ‘ê·¼ ê±°ë¶€
âŒ ë‹¤ë¥¸ ì´ë¦„ì˜ Service Account â†’ ì ‘ê·¼ ê±°ë¶€<br />âŒ ì¼ë°˜ ì‚¬ìš©ìë‚˜ ë‹¤ë¥¸ ì¸ì¦ ë°©ì‹ â†’ ì ‘ê·¼ ê±°ë¶€
âœ… karpenter ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ karpenter Service Accountë§Œ â†’ ì ‘ê·¼ í—ˆìš©</p>
</blockquote>
<h4 id="aws-account-id-ê°€ì ¸ì˜¤ê¸°">AWS Account ID ê°€ì ¸ì˜¤ê¸°</h4>
<pre><code>data &quot;aws_caller_identity&quot; &quot;current&quot; {}</code></pre><p>í˜„ì¬ AWS ê³„ì •ì˜ IDë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì™€ ì •ì±…ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>
<h4 id="karpenter-controller-policy">Karpenter Controller Policy</h4>
<pre><code>resource &quot;aws_iam_policy&quot; &quot;karpenter_controller_policy&quot; {
  name   = &quot;KarpenterControllerPolicy&quot;
  policy = templatefile(&quot;karpenter-controller-policy.json&quot;, {
    AWS_PARTITION  = &quot;aws&quot;
    AWS_ACCOUNT_ID = data.aws_caller_identity.current.account_id
    AWS_REGION     = &quot;ap-northeast-2&quot;
    CLUSTER_NAME   = aws_eks_cluster.main.name
  })
}</code></pre><p><strong>templatefile í•¨ìˆ˜</strong>:</p>
<ul>
<li>ì™¸ë¶€ JSON íŒŒì¼(karpenter-controller-policy.json)ì„ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©
ë³€ìˆ˜ë“¤ì„ ë™ì ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ ì •ì±… ìƒì„±</li>
</ul>
<p><strong>ì£¼ìš” ê¶Œí•œ (ì¼ë°˜ì ìœ¼ë¡œ í¬í•¨ë˜ëŠ” ê²ƒë“¤)</strong>:</p>
<ul>
<li>EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±/ì‚­ì œ/ìˆ˜ì •</li>
<li>Auto Scaling Group ê´€ë¦¬</li>
<li>IAM ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ ì—°ê²°</li>
<li>íƒœê·¸ ê´€ë¦¬</li>
<li>EKS ë…¸ë“œ ê·¸ë£¹ ê´€ë¦¬</li>
</ul>
<h3 id="2-ì„œë¸Œë„·ê³¼-ë³´ì•ˆê·¸ë£¹ì—-karpenter-íƒœê·¸-ì¶”ê°€">2. ì„œë¸Œë„·ê³¼ ë³´ì•ˆê·¸ë£¹ì— Karpenter íƒœê·¸ ì¶”ê°€</h3>
<pre><code>resource &quot;aws_ec2_tag&quot; &quot;private_subnet_tags&quot; {
  count       = length(local.private_subnet_ids)
  resource_id = local.private_subnet_ids[count.index]
  key         = &quot;karpenter.sh/discovery&quot;
  value       = aws_eks_cluster.main.name
}</code></pre><p>Karpenterê°€ ë…¸ë“œë¥¼ ë°°í¬í•  ì„œë¸Œë„·ì„ í”„ë¼ì´ë¹— ì„œë¸Œë„·ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ íƒœê·¸ ì¶”ê°€
KarpenterëŠ” ì´ íƒœê·¸ë¥¼ í†µí•´ ì–´ëŠ ì„œë¸Œë„·ì— ìƒˆ ë…¸ë“œë¥¼ ìƒì„±í• ì§€ ê²°ì •</p>
<h3 id="3-karpneter-helm-ì„¤ì¹˜">3. karpneter helm ì„¤ì¹˜</h3>
<pre><code>Karpenter v1.6.0 ì„¤ì¹˜
helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.6.0 \
--namespace karpenter \
--create-namespace \
--set &quot;serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn=${aws_iam_role.karpenter_controller_role.arn}&quot; \
--set &quot;settings.clusterName=${aws_eks_cluster.main.name}&quot; \
--set &quot;settings.clusterEndpoint=${aws_eks_cluster.main.endpoint}&quot; \
--set &quot;settings.defaultInstanceProfile=${aws_iam_instance_profile.eks_node_profile.name}&quot; \
--set &quot;tolerations[0].key=karpenter.sh/unschedulable&quot; \ â†’ &quot;êµì²´ ì˜ˆì • ë…¸ë“œì—ì„œë„ ì‹¤í–‰í•  ìˆ˜ ìˆì–´&quot;
--set &quot;tolerations[0].operator=Exists&quot; \ â†’ &quot;ê°’ì´ ë­ë“  ìƒê´€ì—†ì–´, ìœ ì—°í•˜ê²Œ ëŒ€ì‘í• ê²Œ&quot;
--set &quot;tolerations[0].effect=NoSchedule&quot; \ â†’ &quot;ìŠ¤ì¼€ì¤„ë§ ì œí•œì€ ê´œì°®ì§€ë§Œ ê°•ì œ ì¶•ì¶œì€ ì‹«ì–´&quot;
--set &quot;replicas=1&quot; \ â†’ &quot;ë‚˜ í˜¼ìì„œë„ ì¶©ë¶„í•´, ë¦¬ì†ŒìŠ¤ ì•„ë¼ì&quot;
--set &quot;topologySpreadConstraints[0].maxSkew=2&quot; \ â†’ &quot;ì˜ì—­ ê°„ ì°¨ì´ê°€ 2ê°œ ì´í•˜ë©´ ê´œì°®ì•„&quot;
--set &quot;topologySpreadConstraints[0].topologyKey=topology.kubernetes.io/zone&quot; \ â†’ &quot;ê°€ìš©ì˜ì—­ë³„ë¡œ ë¶„ì‚°í•´ì¤˜&quot;
--set &quot;topologySpreadConstraints[0].whenUnsatisfiable=ScheduleAnyway&quot; \ â†’ &quot;ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ì¼ë‹¨ ì‹¤í–‰í•˜ëŠ” ê²Œ ì¤‘ìš”í•´&quot;
--set &quot;topologySpreadConstraints[0].labelSelector.matchLabels.app\\.kubernetes\\.io/name=karpenter&quot; â†’ &quot;ì´ ì •ì±…ì€ ë‚˜(Karpenter)ì—ê²Œë§Œ ì ìš©í•´ì¤˜&quot;</code></pre><h4 id="tolerations0keykarpentershunschedulable">tolerations[0].key=karpenter.sh/unschedulable</h4>
<pre><code>ì‹¤ì œìƒí™©ì˜ˆì‹œ:

ìƒí™©: ë…¸ë“œ Aê°€ êµì²´ ì˜ˆì • ìƒíƒœ
  â†“
ê¸°ì¡´ Podë“¤: &quot;ì´ ë…¸ë“œ ìœ„í—˜í•´, ë‹¤ë¥¸ ê³³ìœ¼ë¡œ í”¼í•˜ì!&quot; ğŸƒâ€â™‚ï¸
  â†“  
Karpenter: &quot;ì–´? ê·¸ëŸ¼ ë‚´ê°€ ëˆ„ê°€ ìƒˆ ë…¸ë“œ ë§Œë“¤ì–´ì¤˜?&quot; ğŸ¤”
  â†“
Karpenterê°€ í”¼í•´ë²„ë¦¬ë©´: ìƒˆ ë…¸ë“œ ìƒì„± ë¶ˆê°€! ğŸ˜±

ì™œ í•„ìš”í•œê°€?
ì¼ë°˜ ì•± Pod: êµì²´ ì˜ˆì • ë…¸ë“œ í”¼í•¨ (ë°ì´í„° ë³´í˜¸)
Karpenter: êµì²´ ì˜ˆì • ë…¸ë“œì—ì„œë„ ì‹¤í–‰ (ì„œë¹„ìŠ¤ ì—°ì†ì„±)
</code></pre><h4 id="tolerations0operatorexists">tolerations[0].operator=Exists</h4>
<pre><code>ì‹¤ì œìƒí™©ì˜ˆì‹œ:

ì‹œë‚˜ë¦¬ì˜¤ 1: Taint ê°’ì´ &quot;replacing&quot;
ë…¸ë“œ ìƒíƒœ: karpenter.sh/unschedulable=replacing

ì‹œë‚˜ë¦¬ì˜¤ 2: Taint ê°’ì´ &quot;draining&quot;  
ë…¸ë“œ ìƒíƒœ: karpenter.sh/unschedulable=draining

ì‹œë‚˜ë¦¬ì˜¤ 3: Taint ê°’ì´ &quot;upgrading&quot;
ë…¸ë“œ ìƒíƒœ: karpenter.sh/unschedulable=upgrading

ë§Œì•½ íŠ¹ì • ê°’ë§Œ í—ˆìš©í•œë‹¤ë©´?
ì´ë ‡ê²Œ ì„¤ì •í–ˆë‹¤ë©´:
tolerations:
- key: karpenter.sh/unschedulable
  value: &quot;replacing&quot;  # íŠ¹ì • ê°’ë§Œ!
  operator: Equal

ë¬¸ì œ ë°œìƒ:
- &quot;draining&quot; ìƒíƒœì—ì„œëŠ” ì‹¤í–‰ ë¶ˆê°€ âŒ
- &quot;upgrading&quot; ìƒíƒœì—ì„œë„ ì‹¤í–‰ ë¶ˆê°€ âŒ
â†’ Karpenter ì„œë¹„ìŠ¤ ì¤‘ë‹¨! ğŸ˜±

Existsì˜ ì¥ì :
operator: Exists = &quot;í‚¤ë§Œ ìˆìœ¼ë©´ ê°’ì€ ë­ë“  OK!&quot;
â†’ ëª¨ë“  êµì²´ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€ì‘ ê°€ëŠ¥ âœ…</code></pre><h4 id="tolerations0effectnoschedule">tolerations[0].effect=NoSchedule</h4>
<pre><code>NoSchedule vs NoExecute ì°¨ì´:
- NoSchedule: &quot;ìƒˆë¡œ ìŠ¤ì¼€ì¤„ë§ ì•ˆ í•´ì¤„ë˜&quot; (ê¸°ì¡´ ê²ƒì€ ê³„ì† ì‹¤í–‰)
- NoExecute:  &quot;ê¸°ì¡´ ê²ƒë„ ê°•ì œë¡œ ì«“ì•„ë‚¼ë˜&quot; (ì¦‰ì‹œ ì¢…ë£Œ)

ì‹¤ì œìƒí™©ì˜ˆì‹œ:
ìƒí™©: ë…¸ë“œì— NoExecute Taint ì ìš©ë¨
  â†“
Karpenter Pod: ì¦‰ì‹œ ê°•ì œ ì¢…ë£Œ ğŸ˜µ
  â†“
ìƒˆë¡œìš´ ìŠ¤ì¼€ì¼ë§ ìš”ì²­: &quot;Karpenter ì–´ë””ê°”ì–´?&quot; ğŸ¤·â€â™‚ï¸
  â†“  
ê²°ê³¼: í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§ ë§ˆë¹„! ğŸ˜±

ì™œ NoScheduleë§Œ?
NoSchedule Toleration:
- ê¸°ì¡´ Karpenter Pod: ê³„ì† ì‹¤í–‰ âœ…
- ìƒˆ Karpenter Pod: ë‹¤ë¥¸ ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§ âœ…
- ì„œë¹„ìŠ¤ ì—°ì†ì„±: ë³´ì¥ë¨ âœ…</code></pre><h4 id="replicas1">replicas=1</h4>
<pre><code>Karpenterì˜ ì£¼ìš” ì—…ë¬´:
1. í´ëŸ¬ìŠ¤í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§ 
2. ìŠ¤ì¼€ì¼ë§ ê²°ì •  
3. AWS API í˜¸ì¶œ 
4. ë…¸ë“œ ìƒì„±/ì‚­ì œ ì§€ì‹œ 

ì™œ í•œê°œë©´ ì¶©ë¶„í• ê¹Œ?
âŒ ì—¬ëŸ¬ ê°œê°€ ë™ì‹œì— ê°™ì€ ì¼ì„ í•˜ë©´:
   Pod A: &quot;ë…¸ë“œ 3ê°œ ë” í•„ìš”í•´!&quot;
   Pod B: &quot;ë…¸ë“œ 3ê°œ ë” í•„ìš”í•´!&quot;  
   ê²°ê³¼: 6ê°œ ìƒì„± â†’ ìì› ë‚­ë¹„ ğŸ˜±

âœ… 1ê°œê°€ ëª¨ë“  ê±¸ ê´€ë¦¬í•˜ë©´:
   ë‹¨ì¼ ê²°ì •ì: &quot;ë…¸ë“œ 3ê°œë§Œ ì¶”ê°€í•˜ì&quot;
   ê²°ê³¼: ì •í™•íˆ 3ê°œ ìƒì„± â†’ íš¨ìœ¨ì !

Karpenterì˜ ê³ ê°€ìš©ì„±ì€?
í•œê°œë¡œ ìš´ì˜ì¤‘ì¸ Karpenter Pod ì£½ìœ¼ë©´?
â†’ Kubernetes Deploymentê°€ ìë™ìœ¼ë¡œ ì¬ì‹œì‘
â†’ ì ê¹ ì¤‘ë‹¨ë˜ì§€ë§Œ ê³§ ë³µêµ¬ âœ…

vs ì—¬ëŸ¬ ê°œ Karpenter Pod ìš´ì˜ ì‹œ:
â†’ ë³µì¡í•œ ë¦¬ë” ì„ ì¶œ í•„ìš”
â†’ ë™ì‹œì„± ì œì–´ ë³µì¡
â†’ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„</code></pre><h4 id="maxskew2">maxSkew=2</h4>
<pre><code>ì‹¤ì œ ê°€ìš©ì˜ì—­ ì‹œë‚˜ë¦¬ì˜¤:
í˜„ì¬ ìƒí™©:
- AZ-A: Karpenter Pod 0ê°œ
- AZ-B: Karpenter Pod 1ê°œ  
- AZ-C: Karpenter Pod 0ê°œ

ì°¨ì´: ìµœëŒ€ 1ê°œ (1-0=1) â†’ maxSkew=2 ì¡°ê±´ ë§Œì¡± âœ…

ì™œ 2ê°œê¹Œì§€ í—ˆìš©?
ë„ˆë¬´ ì—„ê²©í•œ ê²½ìš° (maxSkew=0):
- AZ-A: 0ê°œ, AZ-B: 1ê°œ â†’ ì°¨ì´ 1ê°œ âŒ
- ìƒˆ Podì„ AZ-Aë‚˜ AZ-Cì— ê°•ì œë¡œ ë°°ì¹˜
- í•˜ì§€ë§Œ ê·¸ ì˜ì—­ì— ì ì ˆí•œ ë…¸ë“œê°€ ì—†ì„ ìˆ˜ë„...

ì ë‹¹íˆ ìœ ì—°í•œ ê²½ìš° (maxSkew=2):
- AZ-A: 0ê°œ, AZ-B: 2ê°œ, AZ-C: 0ê°œ â†’ ì°¨ì´ 2ê°œ âœ…  
- ì—¬ì „íˆ ë¶„ì‚°ë˜ì–´ ìˆê³ , ìŠ¤ì¼€ì¤„ë§ë„ ìœ ì—°í•¨

ì‹¤ì œ ì¥ì•  ìƒí™©:
AZ-A ì¥ì•  ë°œìƒ! ğŸ”¥
- maxSkew=0: Podì´ AZ-Aì— ì—†ì–´ì„œ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ âŒ
- maxSkew=2: AZ-Bë‚˜ AZ-Cì— ì—¬ìœ ë¡­ê²Œ ë°°ì¹˜ âœ…</code></pre><h4 id="whenunsatisfiablescheduleanyway">whenUnsatisfiable=ScheduleAnyway</h4>
<pre><code>ì™„ë²½í•œ ë¶„ì‚°ì´ ë¶ˆê°€ëŠ¥í•œ ìƒí™©ë“¤:

ìƒí™© 1: ë…¸ë“œ ë¶€ì¡±
í´ëŸ¬ìŠ¤í„° êµ¬ì„±:
- AZ-A: ë…¸ë“œ ìˆìŒ âœ…
- AZ-B: ë…¸ë“œ ì—†ìŒ âŒ  
- AZ-C: ë…¸ë“œ ì—†ìŒ âŒ

ì™„ë²½í•œ ë¶„ì‚° ìš”êµ¬ ì‹œ: ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨ âŒ
ìœ ì—°í•œ ì •ì±…: AZ-Aì—ë¼ë„ ë°°ì¹˜ âœ…

ìƒí™© 2: ë¦¬ì†ŒìŠ¤ ë¶€ì¡±
ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­:
- Karpenter Pod: CPU 500m, Memory 512Mi í•„ìš”

í˜„ì¬ ìƒí™©:
- AZ-A: ë¦¬ì†ŒìŠ¤ ì¶©ë¶„ âœ…
- AZ-B: ë¦¬ì†ŒìŠ¤ ë¶€ì¡± âŒ
- AZ-C: ë¦¬ì†ŒìŠ¤ ë¶€ì¡± âŒ

ê²°ê³¼: AZ-Aì— ë°°ì¹˜ë˜ì–´ ì„œë¹„ìŠ¤ ê³„ì† ì œê³µ âœ…

DoNotSchedule vs ScheduleAnyway:
DoNotSchedule: &quot;ì™„ë²½í•˜ì§€ ì•Šìœ¼ë©´ ì•„ì˜ˆ ì‹¤í–‰ ì•ˆ í•´!&quot;
â†’ Karpenter ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ê°€ëŠ¥ ğŸ˜±

ScheduleAnyway: &quot;ë¶ˆì™„ì „í•´ë„ ì¼ë‹¨ ì‹¤í–‰í•´ì„œ ì„œë¹„ìŠ¤ ìœ ì§€!&quot;  
â†’ ê³ ê°€ìš©ì„± ìš°ì„  âœ…</code></pre><h4 id="labelselectormatchlabelsappkubernetesionamekarpenter">labelSelector.matchLabels.app\.kubernetes\.io/name=karpenter</h4>
<pre><code>&quot;ë‹¤ë¥¸ Podë“¤ì´ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆë„ë¡ ë…¸ë“œë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒ&quot;

ë”°ë¼ì„œ: 
1. ë‹¤ë¥¸ Podë“¤ì´ í”¼í•˜ëŠ” ë…¸ë“œì—ì„œë„ â†’ ë‚˜ëŠ” ì‹¤í–‰ë˜ì–´ì•¼ í•¨
2. ì–´ë–¤ ìƒí™©ì—ì„œë“  â†’ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•´ì•¼ í•¨  
3. ê°•ì œ ì¢…ë£Œë˜ë©´ ì•ˆ ë˜ê³  â†’ ì„œë¹„ìŠ¤ ì—°ì†ì„± ìœ ì§€
4. íš¨ìœ¨ì ìœ¼ë¡œ â†’ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ê¸ˆì§€
5. ê°€ìš©ì„±ì„ ìœ„í•´ â†’ ì ë‹¹í•œ ë¶„ì‚°
6. ì™„ë²½í•˜ì§€ ì•Šì•„ë„ â†’ ì„œë¹„ìŠ¤ê°€ ë¨¼ì €</code></pre><h4 id="ê²°êµ­-ì´-ëª¨ë“ -ì„¤ì •ì€-karpenterê°€-ì–´ë–¤-ìƒí™©ì—ì„œë“ -ì•ˆì •ì ìœ¼ë¡œ-ì‹¤í–‰ë˜ì–´-í´ëŸ¬ìŠ¤í„°-ì „ì²´ì˜-ì•ˆì •ì„±ê³¼-ë¹„ìš©-íš¨ìœ¨ì„±ì„-ë³´ì¥í•˜ê¸°-ìœ„í•¨ì…ë‹ˆë‹¤">ê²°êµ­ ì´ ëª¨ë“  ì„¤ì •ì€ &quot;Karpenterê°€ ì–´ë–¤ ìƒí™©ì—ì„œë“  ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´, í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ì•ˆì •ì„±ê³¼ ë¹„ìš© íš¨ìœ¨ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•¨&quot;ì…ë‹ˆë‹¤!</h4>
<h1 id="3ë‹¨ê³„-karpenter-nodepool--ec2nodeclass">3ë‹¨ê³„: Karpenter NodePool &amp; EC2NodeClass</h1>
<pre><code>############################################
# Karpenter v1.6.0 NodePool &amp; EC2NodeClass ìƒì„±
############################################
resource &quot;null_resource&quot; &quot;karpenter_nodepool&quot; {
  depends_on = [null_resource.install_karpenter]

  connection {
    type        = &quot;ssh&quot;
    host        = aws_instance.bastion.public_ip
    user        = &quot;ec2-user&quot;
    private_key = file(&quot;C:/.ssh/bastion-key.pem&quot;)
  }

  provisioner &quot;remote-exec&quot; {
    inline = [
      &lt;&lt;-EOT
      echo &quot;Waiting for Karpenter controller to become available...&quot;
      kubectl wait --namespace=karpenter deployment/karpenter --for=condition=Available=True --timeout=10m

      echo &quot;Waiting for Karpenter CRDs to be established...&quot;
      # CRDê°€ ë“±ë¡ë  ë•Œê¹Œì§€ ëŒ€ê¸°
      kubectl wait --for condition=established --timeout=300s crd/nodepools.karpenter.sh
      kubectl wait --for condition=established --timeout=300s crd/ec2nodeclasses.karpenter.k8s.aws

      # CRD ìƒíƒœ í™•ì¸
      echo &quot;Checking CRD status...&quot;
      kubectl get crd | grep karpenter

      # API ë²„ì „ í™•ì¸
      echo &quot;Checking available API versions...&quot;
      kubectl api-resources | grep -E &quot;(nodepool|ec2nodeclass)&quot;

      # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„
      sleep 30

      # EC2NodeClass ìƒì„± (v1 API)
      echo &quot;Creating EC2NodeClass...&quot;
      cat &lt;&lt;EOF | kubectl apply -f -
      apiVersion: karpenter.k8s.aws/v1
      kind: EC2NodeClass
      metadata:
        name: default
      spec:
        # AMI ì„¤ì •
        amiSelectorTerms:
          - alias: al2023@latest # Karpenterê°€ ì œê³µí•˜ëŠ” AL2023 AMI ì‚¬ìš©

        # ì„œë¸Œë„· ì„ íƒ
        subnetSelectorTerms:
          - tags:
              karpenter.sh/discovery: &quot;${aws_eks_cluster.main.name}&quot;

        # ë³´ì•ˆ ê·¸ë£¹ ì„ íƒ
        securityGroupSelectorTerms:
          - tags:
              karpenter.sh/discovery: &quot;${aws_eks_cluster.main.name}&quot;

        # IAM ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼
        instanceProfile: &quot;${aws_iam_instance_profile.eks_node_profile.name}&quot;

        # ì‚¬ìš©ì ë°ì´í„° ìŠ¤í¬ë¦½íŠ¸
        userData: |
          #!/bin/bash
          /etc/eks/bootstrap.sh ${aws_eks_cluster.main.name}

          # ì¶”ê°€ ì„¤ì •
          echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf
          sysctl -p /etc/sysctl.conf

        # ë¸”ë¡ ë””ë°”ì´ìŠ¤ ë§¤í•‘
        blockDeviceMappings:
          - deviceName: /dev/xvda
            ebs:
              volumeSize: 20Gi
              volumeType: gp3
              deleteOnTermination: true

        # ë©”íƒ€ë°ì´í„° ì„œë¹„ìŠ¤ ì„¤ì •
        metadataOptions:
          httpEndpoint: enabled
          httpProtocolIPv6: disabled
          httpPutResponseHopLimit: 2
          httpTokens: required
      EOF

      # EC2NodeClass ìƒì„± í™•ì¸
      if kubectl get ec2nodeclass default; then
        echo &quot;EC2NodeClass created successfully&quot;
      else
        echo &quot;Failed to create EC2NodeClass&quot;
        exit 1
      fi

      # ì ì‹œ ëŒ€ê¸°
      sleep 10

      # NodePool ìƒì„± (v1 API)
      echo &quot;Creating NodePool...&quot;
      cat &lt;&lt;EOF | kubectl apply -f -
      apiVersion: karpenter.sh/v1
      kind: NodePool
      metadata:
        name: default
      spec:
        # ë…¸ë“œí´ë˜ìŠ¤ ì°¸ì¡°
        template:
          metadata:
            labels:
              intent: apps
              nodepool: default
          spec:
            # ë…¸ë“œ ìš”êµ¬ì‚¬í•­
            requirements:
              - key: kubernetes.io/arch
                operator: In
                values: [&quot;amd64&quot;]
              - key: karpenter.sh/capacity-type
                operator: In
                values: [&quot;on-demand&quot;]
              - key: node.kubernetes.io/instance-type
                operator: In
                values: [&quot;t3.small&quot;]

            # EC2NodeClass ì°¸ì¡°
            nodeClassRef:
              group: karpenter.k8s.aws
              kind: EC2NodeClass
              name: default

        # ë¦¬ì†ŒìŠ¤ ì œí•œ
        limits:
          cpu: 20
          memory: 40Gi

        # ë…¸ë“œ ì¶•ì¶œ ì •ì±…
        disruption:
          # ë¹ˆ ë…¸ë“œ ì¶•ì¶œ ì‹œê°„
          consolidationPolicy: WhenEmptyOrUnderutilized
          consolidateAfter: 5m
      EOF

      # NodePool ìƒì„± í™•ì¸
      if kubectl get nodepool default; then
        echo &quot;NodePool created successfully&quot;
      else
        echo &quot;Failed to create NodePool&quot;
        exit 1
      fi

      echo &quot;Waiting for resources to be created...&quot;
      sleep 15

      echo &quot;Karpenter v1.6.0 NodePool and EC2NodeClass created successfully&quot;

      # ìƒì„±ëœ ë¦¬ì†ŒìŠ¤ í™•ì¸
      echo &quot;Checking created resources...&quot;
      kubectl get nodepools -o wide
      kubectl get ec2nodeclasses -o wide

      EOT
    ]
  }
}</code></pre><h2 id="crd-custom-resource-definition">CRD (Custom Resource Definition)</h2>
<h3 id="crdë€">CRDë€?</h3>
<ul>
<li>&quot;ìƒˆë¡œìš´ ì–¸ì–´ ì‚¬ì „ ë§Œë“¤ê¸°&quot; Kubernetesë¥¼ ì–¸ì–´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.</li>
</ul>
<p><strong>ê¸°ë³¸ Kubernetes ë‹¨ì–´ë“¤ (ë‚´ì¥ ë¦¬ì†ŒìŠ¤):</strong></p>
<ul>
<li>Pod: &quot;ì»¨í…Œì´ë„ˆ ì‹¤í–‰&quot;</li>
<li>Service: &quot;ë„¤íŠ¸ì›Œí¬ ì—°ê²°&quot;</li>
<li>Deployment: &quot;ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬&quot;</li>
<li>Node: &quot;ì„œë²„&quot;</li>
</ul>
<p><strong>CRD = &quot;ìƒˆë¡œìš´ ë‹¨ì–´ ì •ì˜ì„œ&quot;:</strong></p>
<ul>
<li>NodePool: &quot;ë…¸ë“œ ê·¸ë£¹ ì •ì±…&quot; (Karpenterê°€ ë§Œë“  ìƒˆ ë‹¨ì–´)</li>
<li>EC2NodeClass: &quot;AWS ë…¸ë“œ ì„¤ì •&quot; (Karpenterê°€ ë§Œë“  ìƒˆ ë‹¨ì–´)</li>
</ul>
<h3 id="karpenterì—ì„œ-crdê°€-ì¤‘ìš”í•œ-ì´ìœ ">Karpenterì—ì„œ CRDê°€ ì¤‘ìš”í•œ ì´ìœ </h3>
<h4 id="1-karpenter-controller-ì‘ë™-ë°©ì‹">1. Karpenter Controller ì‘ë™ ë°©ì‹</h4>
<pre><code>Karpenter Controller ì‹œì‘
        â†“
&quot;NodePool ë¦¬ì†ŒìŠ¤ ë³€í™” ê°ì§€í•˜ê² ì–´!&quot;
        â†“
CRD ìˆë‚˜ í™•ì¸
        â†“
âŒ CRD ì—†ìŒ: &quot;NodePoolì´ ë­”ì§€ ëª°ë¼ì„œ ê°ì§€í•  ìˆ˜ ì—†ì–´!&quot; â†’ ì¤‘ë‹¨
âœ… CRD ìˆìŒ: &quot;NodePool ë³€í™”ë¥¼ ê°ì§€í•˜ê³  ë°˜ì‘í•˜ê² ì–´!&quot; â†’ ì •ìƒ ì‘ë™</code></pre><h4 id="2-ì‹¤ì œ-ë…¸ë“œ-ìƒì„±-í”„ë¡œì„¸ìŠ¤">2. ì‹¤ì œ ë…¸ë“œ ìƒì„± í”„ë¡œì„¸ìŠ¤</h4>
<pre><code>ì‚¬ìš©ìê°€ NodePool ìƒì„±/ìˆ˜ì •
        â†“
Kubernetes API Server: &quot;NodePool ë¦¬ì†ŒìŠ¤ê°€ ë³€ê²½ë˜ì—ˆë„¤&quot;
        â†“
Karpenter Controller: &quot;ë³€ê²½ ê°ì§€! ìƒˆë¡œìš´ ë…¸ë“œê°€ í•„ìš”í•œê°€?&quot;
        â†“
EC2NodeClass ì°¸ì¡°: &quot;ì–´ë–¤ ìŠ¤í™ì˜ ì„œë²„ë¥¼ ë§Œë“¤ì§€ í™•ì¸&quot;
        â†“
AWS EC2 API í˜¸ì¶œ: &quot;ì‹¤ì œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±&quot;</code></pre><h4 id="3-crdê°€-ì—†ë‹¤ë©´">3. CRDê°€ ì—†ë‹¤ë©´?</h4>
<pre><code>âŒ &quot;NodePoolì´ ë­”ì§€ ëª°ë¼ìš”&quot;
âŒ Controller ì•„ì˜ˆ ì‹œì‘ ì•ˆë¨
âŒ ë…¸ë“œ ìƒì„± ë¶ˆê°€ëŠ¥</code></pre><h3 id="crdì˜-ì‹¤í–‰ì„-ê¸°ë‹¤ë¦¼">CRDì˜ ì‹¤í–‰ì„ ê¸°ë‹¤ë¦¼</h3>
<pre><code>echo &quot;Waiting for Karpenter CRDs to be established...&quot;
kubectl wait --for condition=established --timeout=300s crd/nodepools.karpenter.sh
kubectl wait --for condition=established --timeout=300s crd/ec2nodeclasses.karpenter.k8s.aws</code></pre><p><strong>ì™œ ê¸°ë‹¤ë ¤ì•¼í• ê¹Œ?</strong></p>
<ol>
<li>Karpenter Helm ì„¤ì¹˜ â†’ CRD ìƒì„± ì‹œì‘</li>
<li>CRD ì™„ì „ ë“±ë¡ê¹Œì§€ ì‹œê°„ ì†Œìš” (ë³´í†µ 30ì´ˆ~2ë¶„)</li>
<li>CRD ì¤€ë¹„ ì™„ë£Œ â†’ NodePool/EC2NodeClass ìƒì„± ê°€ëŠ¥<h2 id="ec2nodeclass">EC2NodeClass</h2>
<h3 id="ec2nodeclass--ì–´ë–»ê²Œ-ë§Œë“¤ê¹Œ-how">EC2NodeClass = &quot;ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ?&quot; (HOW)</h3>
<pre><code># EC2NodeClassëŠ” EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë°©ë²•ì„ ì •ì˜
spec:
amiSelectorTerms: [...] # ì–´ë–¤ AMI ì‚¬ìš©í• ì§€
userData: |             # ë¶€íŒ… ì‹œ ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸
 /etc/eks/bootstrap.sh
blockDeviceMappings:    # ë””ìŠ¤í¬ êµ¬ì„±
 volumeSize: 20Gi
metadataOptions:        # ë³´ì•ˆ ì„¤ì •
 httpTokens: required</code></pre><h3 id="ec2nodeclassê°€-ë‹´ë‹¹í•˜ëŠ”-ê²ƒë“¤">EC2NodeClassê°€ ë‹´ë‹¹í•˜ëŠ” ê²ƒë“¤</h3>
```
AWS EC2 ê´€ë ¨ ëª¨ë“  ì„¤ì •:</li>
</ol>
<ul>
<li>AMI ì„ íƒ (ìš´ì˜ì²´ì œ)</li>
<li>ë³´ì•ˆê·¸ë£¹ (ë„¤íŠ¸ì›Œí¬ ë°©í™”ë²½)</li>
<li>ì„œë¸Œë„· (ë„¤íŠ¸ì›Œí¬ ìœ„ì¹˜)</li>
<li>IAM ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œíŒŒì¼ (ê¶Œí•œ)</li>
<li>User Data (ë¶€íŒ… ìŠ¤í¬ë¦½íŠ¸)</li>
<li>EBS ë³¼ë¥¨ (ìŠ¤í† ë¦¬ì§€)</li>
<li>EC2 ë©”íƒ€ë°ì´í„° ì„¤ì •</li>
</ul>
<p>â†’ &quot;ë¬¼ë¦¬ì  ì„œë²„ë¥¼ ì–´ë–»ê²Œ êµ¬ì„±í•  ê²ƒì¸ê°€&quot;</p>
<pre><code>## NodePool
### NodePool = &quot;ì–¸ì œ, ì–¼ë§ˆë‚˜ ë§Œë“¤ê¹Œ?&quot; (WHEN &amp; HOW MANY)</code></pre><h1 id="nodepoolì€-ìŠ¤ì¼€ì¼ë§-ì¡°ê±´ê³¼-ì œì•½ì„-ì •ì˜">NodePoolì€ ìŠ¤ì¼€ì¼ë§ ì¡°ê±´ê³¼ ì œì•½ì„ ì •ì˜</h1>
<p>spec:
  requirements:           # ì–´ë–¤ ì¡°ê±´ì˜ ë…¸ë“œê°€ í•„ìš”í•œì§€
    - key: node.kubernetes.io/instance-type
      values: [&quot;t3.small&quot;]
  limits:                 # ìµœëŒ€ ì–¼ë§ˆë‚˜ ìƒì„±í• ì§€
    cpu: 20
  disruption:             # ì–¸ì œ ì‚­ì œí• ì§€
    consolidateAfter: 5m</p>
<pre><code>### NodePoolì´ ë‹´ë‹¹í•˜ëŠ” ê²ƒë“¤</code></pre><p>Kubernetes ìŠ¤ì¼€ì¼ë§ ê´€ë ¨ ëª¨ë“  ì •ì±…:</p>
<ul>
<li>ë…¸ë“œ ìš”êµ¬ì‚¬í•­ (ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…, ì•„í‚¤í…ì²˜ ë“±)</li>
<li>ë¦¬ì†ŒìŠ¤ ì œí•œ (ìµœëŒ€ CPU/ë©”ëª¨ë¦¬)</li>
<li>ìŠ¤ì¼€ì¼ë§ ì •ì±… (ì–¸ì œ ìƒì„±/ì‚­ì œ)</li>
<li>ë…¸ë“œ ë¼ë²¨ë§</li>
<li>Taints/Tolerations</li>
</ul>
<p>â†’ &quot;ì–¸ì œ ì–¼ë§ˆë‚˜ ë§Œë“¤ê³  ê´€ë¦¬í•  ê²ƒì¸ê°€&quot;</p>
<pre><code>
## 1:N ê´€ê³„ ì´í•´
### í•˜ë‚˜ì˜ EC2NodeClass, ì—¬ëŸ¬ NodePool ê°€ëŠ¥</code></pre><p>í•˜ë‚˜ì˜ EC2NodeClass:</p>
<ul>
<li>í‘œì¤€ AMI ì„¤ì •</li>
<li>í‘œì¤€ ë³´ì•ˆ ì„¤ì •  </li>
<li>í‘œì¤€ ë„¤íŠ¸ì›Œí¬ ì„¤ì •</li>
</ul>
<p>ì—¬ëŸ¬ NodePool:</p>
<ul>
<li>ê°œë°œìš© (ì‘ì€ ì¸ìŠ¤í„´ìŠ¤)</li>
<li>ìš´ì˜ìš© (í° ì¸ìŠ¤í„´ìŠ¤)  </li>
<li>GPUìš© (GPU ì¸ìŠ¤í„´ìŠ¤)</li>
<li>Spotìš© (ë¹„ìš© ì ˆì•½)</li>
</ul>
<p>â†’ ê³µí†µ ì¸í”„ë¼ ì„¤ì •ì€ ì¬ì‚¬ìš©í•˜ë©´ì„œ 
  ìš©ë„ë³„ ìŠ¤ì¼€ì¼ë§ ì •ì±…ì€ ë¶„ë¦¬</p>
<p>```</p>
<h2 id="ec2nodeclass-vs-nodepool">EC2NodeClass VS NodePool</h2>
<table>
<thead>
<tr>
<th align="left">êµ¬ë¶„</th>
<th align="left">EC2NodeClass</th>
<th align="left">NodePool</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>ì—­í• </strong></td>
<td align="left">AWS ì¸í”„ë¼ í…œí”Œë¦¿</td>
<td align="left">Kubernetes ìŠ¤ì¼€ì¼ë§ ì •ì±…</td>
</tr>
<tr>
<td align="left"><strong>ê´€ì‹¬ì‚¬</strong></td>
<td align="left">HOW (ì–´ë–»ê²Œ ë§Œë“¤ê¹Œ)</td>
<td align="left">WHEN/HOW MANY (ì–¸ì œ/ì–¼ë§ˆë‚˜)</td>
</tr>
<tr>
<td align="left"><strong>ì„¤ì • ë‚´ìš©</strong></td>
<td align="left">AMI, ë³´ì•ˆê·¸ë£¹, ì„œë¸Œë„·, ìŠ¤í† ë¦¬ì§€</td>
<td align="left">ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…, ë¦¬ì†ŒìŠ¤ ì œí•œ, ìŠ¤ì¼€ì¼ë§</td>
</tr>
<tr>
<td align="left"><strong>ë³€ê²½ ë¹ˆë„</strong></td>
<td align="left">ë‚®ìŒ (ì¸í”„ë¼ í‘œì¤€)</td>
<td align="left">ë†’ìŒ (ì›Œí¬ë¡œë“œë³„ ìš”êµ¬ì‚¬í•­)</td>
</tr>
<tr>
<td align="left"><strong>ì¬ì‚¬ìš©ì„±</strong></td>
<td align="left">ë†’ìŒ (ì—¬ëŸ¬ NodePoolì´ ì°¸ì¡°)</td>
<td align="left">ë‚®ìŒ (íŠ¹ì • ìš©ë„)</td>
</tr>
</tbody></table>